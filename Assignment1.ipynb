{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2l_u4kPQIbl7",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-ff06de8ceaeddcd2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Building A Neural Network From Scratch (15 Marks):\n",
    "In this assignment, you will build a neural network from scratch using NumPy. You will implement functions to build a neural network with as many layers as you want. You will also use this neural network to build a classifier for the [Credit Approval Dataset](https://archive.ics.uci.edu/ml/datasets/credit+approval) after performing data cleaning and preprocessing tasks yourself.\n",
    "<center><img src=\"credit-approval-rubber-stamp.jpg\" width=\"180\" height=\"180\"></center>\n",
    "So, let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "foM4FRogLyx8",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-a6bac1fa2e306d57",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Overview of the Assignment:\n",
    "The assignment asks you to complete several functions, with each function guiding you through the necessary steps to complete it. You will:\n",
    "1. Preprocess the Credit Approval Dataset (like fill in missing values, split into test and train sets, etc).\n",
    "2. Implement various activation functions taught in class.\n",
    "3. Initialize the weight matrices and bias vectors of the neural network depending on its architecture.\n",
    "4. Implement the forward propagation module.\n",
    "5. Compute cross entropy loss function.\n",
    "6. Implement the backward propagation module to compute gradients of loss function w.r.t. weights of the network.\n",
    "7. With the help of batch gradient descent optimizer, update the weights of the neural network using gradients computed in the previous step.\n",
    "8. Repeat steps 3-6 for several epochs to train the neural network on Credit Approval Dataset.\n",
    "9. Report the accuracy of the neural network on test data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "P0_g9plwKpLI",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-b6a249b6e610bd3b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Terminologies used for describing the computations of the Neural Network:\n",
    "* Superscript $[h]$ denotes a quantity associated with the $h^{th}$ layer. \n",
    "    - Example: $a^{[h]}$ is the $h^{th}$ layer activation. $W^{[h]}$ and $b^{[h]}$ are the $h^{th}$ layer parameters.\n",
    "* Superscript $(i)$ denotes a quantity associated with the $i^{th}$ example. \n",
    "    - Example: $y^{(i)}$ is the ground truth label corresponding to the $i^{th}$ training example.\n",
    "\n",
    "<center><img src=\"neural_network_diagram.jpg\" width=\"512\" height=\"200\"></center>\n",
    "\n",
    "* A layer of the neural network consists of:\n",
    "    - a weight matrix $W^{[h]}$of shape (size of current layer, size of previous layer)\n",
    "    - a bias vector $b^{[h]} $of shape (size of current layer, 1)\n",
    "    - an activation function $ g^{[h]} $\n",
    "* Computation in a layer $h$ of the neural network can be described as follows:\n",
    "  - Let $A^{[h-1]}$ be the activations of the previous layer or input data (if current layer is first hidden layer), of shape (size of previous layer, number of examples).\n",
    "  - Let $ Z^{[h]} $ be the pre-activation value. Then, $ Z^{[h]} $ is computed as :\n",
    "  $$ Z^{[h]} = W^{[h]}.A^{[h-1]}+b^{[h]}$$\n",
    "  where $ . $ denotes matrix multiplication.\n",
    "  So, $Z^{[h]}$ will be of shape (size of current layer, number of examples).\n",
    "  - Let $A^{[h]}$ denote post activation value. It is computed as:\n",
    "  $$ A^{[h]} = g^{[h]}(Z^{[h]}) $$\n",
    "  Note that shape of $A^{[h]}$ is same as that of $Z^{[h]}$.\n",
    "  - Above, $h$ is an integer in the range $[1, H]$. $h=H$ denotes the output layer, while $h<H$ denotes a hidden layer.\n",
    "  - Also note that $A^{[0]}$ is nothing but $X$, the input data.\n",
    "* If you wish to learn about neural networks from scratch, you can watch the videos 43 to 57 (as per your convenience/requirement) from this [Youtube playlist](https://www.youtube.com/playlist?list=PLLssT5z_DsK-h9vYZkQkYNWcItqhlRJLN)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IleUSw1AQG5t",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-c2690980b5a080a2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Neural Network Architecture Implementation Details:\n",
    "* The neural network you will build will consist of $H$ layers in total (excluding the input layer). It will have ($H-1$) hidden layers  and an output layer. \n",
    "* The list ```layers_sizes``` (of length $H+1$) contains sizes of input layer (at index $0$), hidden layers (from index $1$ to $H-1$) and output layer (at index $H$). \n",
    "* The list ```activations``` (of length H) is list of strings, representing the activation functions used in the hidden layers and the output layer. Since the Credit Approval Dataset is a classification task, the string at index $H-1$ will always be \"sigmoid\". \n",
    "* The weights of the neural network will be stored in a list ```weights``` (of length $H$), where each element is a list - $[W^{[h]}, b^{[h]}]$, representing the  weights matrix and bias vector of layer $h$ (Both $W^{[h]}$ and  $b^{[h]}$ are numpy arrays).\n",
    "* You will notice that there is a backward function corresponding to each forward function. So, it is essential to store some important values computed in the forward pass that will be useful in the backward pass for computing gradients. A list ```layers_cache``` (of length $H$) is used for this purpose and its details will be described later.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nBUYwxy0PpQ9",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-28729ce7bafc1d5f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### What You Have To Do?\n",
    "* The assignment is divided into 11 sections, out of which you only need to complete the sections 2 to 8.\n",
    "* You will have to complete certain portions of certain functions. Write your code at the comment ```# YOUR CODE HERE``` and remove or comment out the line ```raise NotImplementedError()```. The functions will clearly specify what your code should do. \n",
    "* Do not change the type of any cell and do not create any extra cells in your final submission.\n",
    "* The maximum marks for this assignment are 15 marks. The marks for each function and each section are specified above them. Except at one place in section 2, you will be awarded marks for passing the hidden test-cases and not the sample test-cases.\n",
    "* As evident from the design of sample  test-cases, we have tried to keep the evaluation of each section independent from the other sections as far as possible, so that you get marks proportional to the sections you have implemented.\n",
    "* We have provided you with all the necessary files so that you don't need to download any file.\n",
    "* For any clarifications regarding section 2 (Load, Preprocess and Split the Credit Approval Dataset), contact Abhilash Neog (f2016004@pilani.bits-pilani.ac.in). For queries regarding rest of the sections, contact Parth Patel (f2016150@pilani.bits-pilani.ac.in)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "J4EF1dJ0qCMC",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-741cdb21a20af160",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### 1- Import Packages:\n",
    "**DO NOT** import any package from your side."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EOVnuBKKjzPL",
    "jupyter": {
     "outputs_hidden": true
    },
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-01e2ecfec0cb685a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# do not change code here\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n",
    "pd.set_option('chained_assignment',None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4H1IB_ntqCNT",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-c4180a278c1911e3",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### 2- Load, Preprocess and Split the Credit Approval Dataset (4.5 Marks):\n",
    "\n",
    "The dataset that we are going to work upon is the [Credit Approval Dataset](https://archive.ics.uci.edu/ml/datasets/credit+approval) . The dataset contains credit card application information. Each attribute provides certain information about an applicant's data. The classification task to perform is, given all the information of an applicant, predict whether his application would be approved or not. But, before the classification task is performed, it is necessary that the data is clean and in the right format to feed into the neural network classifier.<br>\n",
    "**Dataset Description**\n",
    "The dataset contains 690 instances with 11 attributes + class/label attribute, whose types are mentioned below:<br>\n",
    "- A1 - nominal\n",
    "- A2 - continuous (real)\n",
    "- A3 - continuous (real)\n",
    "- A4 - nominal\n",
    "- A5 - nominal\n",
    "- A6 - nominal\n",
    "- A7 - nominal\n",
    "- A8 - continuous (real)\n",
    "- A9 - nominal\n",
    "- A10 - continuous (integer)\n",
    "- A11 - continuous (integer)\n",
    "- A12 - nominal - Class attribute (+/-)\n",
    "\n",
    "*There are certain number of missing values in the dataset, which you need to fill, based on some given conditions.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hdWn_tEvflV7",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-afcb65bf0062b41b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### 2.1 Loading the dataset (0.75 Mark)\n",
    "\n",
    "**The tasks to be performed:**<br>\n",
    "  Load the dataset and answer a few queries:\n",
    "- Read the csv (0.25 Mark)\n",
    "- Query 1: find the range/difference between 3rd highest and 3rd lowest attribute value of an attribute(0.25 Mark)\n",
    "- Query 2: find the average value of a given attribute (with 2 decimal places only) (0.25 Mark)\n",
    "\n",
    "*Hint: Pandas has a wide range of pre-defined library functions. Knowing a few would be really helpful to complete below functions easily.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QdaeuKt5qGKJ",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-41eb1c9da9312c1e",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "### 0.25 Mark\n",
    "def load_dataset(filename):\n",
    "    \"\"\"\n",
    "      Input: filename - name of the file to load\n",
    "             the colnames are - A1,A2,A3,A4,A5,A6,A7,A8,A9,A10,A11,A12\n",
    "             Note that the csv file doesn't contain column names, so read it appropriately.\n",
    "      Output: return a pandas dataframe containing the dataset\n",
    "      \"\"\"\n",
    "    ### BEGIN SOLUTION\n",
    "    names = ['A1','A2','A3','A4','A5','A6','A7','A8','A9','A10','A11','A12']\n",
    "    data = pd.read_csv(filename, names = names)\n",
    "    return data\n",
    "    ### END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "load_dataset",
     "locked": true,
     "points": 0.25,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running sample test case\n",
      "Sample test case passed\n"
     ]
    }
   ],
   "source": [
    "# do not change code here\n",
    "### SAMPLE TEST CASE\n",
    "# This is the only sample test case that fetches you marks.\n",
    "# 0.25 marks for passing this sample test case.\n",
    "print(\"Running sample test case\")\n",
    "data = load_dataset(\"crxData.csv\")\n",
    "assert np.allclose(data.shape, (690,12), atol = 1e-5)\n",
    "print(\"Sample test case passed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rG3JFXJkflWK",
    "jupyter": {
     "outputs_hidden": true
    },
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-85af6ec068f34446",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "### 0.25 Mark    \n",
    "def getRange(col):\n",
    "    \"\"\"\n",
    "    Input: col is a pandas series object corresponding to a certain column\n",
    "    Output: return the difference between 3rd highest and 3rd lowest value of col (NOT THE ABSOLUTE VALUE). Round to 2 decimal\n",
    "            places\n",
    "    \"\"\"\n",
    "    ### BEGIN SOLUTION\n",
    "    if(col.dtypes == 'object'):\n",
    "        print('Exception: Column type can''t be of type object')\n",
    "    else:\n",
    "        ans = round(col.nlargest(3).iloc[-1] - col.nsmallest(3).iloc[-1],2)\n",
    "    return ans \n",
    "    ### END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-bf6237fcd0a43737",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running sample test case\n",
      "Sample test case passed\n"
     ]
    }
   ],
   "source": [
    "# do not change code here\n",
    "### SAMPLE TEST CASE\n",
    "print(\"Running sample test case\")\n",
    "diff = 25.21\n",
    "assert np.allclose(diff, getRange(data['A3']), atol = 1e-5)\n",
    "print(\"Sample test case passed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-a62801101b5db4e3",
     "locked": true,
     "points": 0.25,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# do not change code here\n",
    "# Hidden Test cases\n",
    "### BEGIN HIDDEN TESTS\n",
    "dx = np.array([0.68344235, 0.18727672, 0.99035954, 0.3042993 , 0.71736848,\n",
    "       0.97132818, 0.0307962 , 0.81542063, 0.27351922, 0.97098902,\n",
    "       0.37530548, 0.75293285, 0.06873109, 0.26935683, 0.9955386 ,\n",
    "       0.21513214, 0.61042331, 0.72115494, 0.66031082, 0.25435405])\n",
    "dp = pd.Series(dx)\n",
    "assert getRange(dp) == 0.78 #40514580081812\n",
    "### END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3F78xHOCflWa",
    "jupyter": {
     "outputs_hidden": true
    },
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-37a358e03dbe2495",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "### 0.25 Mark\n",
    "def avgprice(col):\n",
    "    \"\"\"\n",
    "    Input: col is a pandas series object corresponding to a certain column\n",
    "    Output: return the average of all the values in that attribute to 2 decimal places\n",
    "    \"\"\"\n",
    "    ### BEGIN SOLUTION\n",
    "    return round(col.mean(),2)\n",
    "    ### END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-92919f00f57a6d57",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running sample test case\n",
      "Sample test case passed\n"
     ]
    }
   ],
   "source": [
    "# do not change code here\n",
    "### SAMPLE TEST CASE\n",
    "print(\"Running sample test case\")\n",
    "avg = 4.76\n",
    "assert np.allclose(avg, avgprice(data['A3']), atol = 1e-5)\n",
    "print(\"Sample test case passed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-db2d0b23b49ba675",
     "locked": true,
     "points": 0.25,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# do not change code here\n",
    "# Hidden Test cases\n",
    "### BEGIN HIDDEN TESTS\n",
    "dx = np.array([0.68344235, 0.18727672, 0.99035954, 0.3042993 , 0.71736848,\n",
    "       0.97132818, 0.0307962 , 0.81542063, 0.27351922, 0.97098902,\n",
    "       0.37530548, 0.75293285, 0.06873109, 0.26935683, 0.9955386 ,\n",
    "       0.21513214, 0.61042331, 0.72115494, 0.66031082, 0.25435405])\n",
    "dp = pd.Series(dx)\n",
    "assert avgprice(dp) == 0.54\n",
    "### END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1PGh3P3sflW3",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-cc944c5a3fd54851",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### 2.2 Filling the missing values (2.75 Marks)\n",
    "\n",
    "**The tasks to be performed:**<br>\n",
    "\n",
    "- For every categorical attribute, replace '?' with the most common string in the neighbourhood (1 Mark)\n",
    "- For every real-valued attribute, replace '?' with the mean of the values in that column (1 Mark)\n",
    "- For every integer-valued attribute, replace '?' with the min value in that column (0.75 Mark)\n",
    "\n",
    "*Explanation for filling categorical attributes:* <br>\n",
    "\n",
    " Find the most common string in the column - cmn_str (say)<br>\n",
    " For every missing value '?', check its previous and next value in the original column (passed as input to the fillCATEGORICAL() function):\n",
    " - if previous does not exist (meaning first index): fill it with the next value (only if next is not '?', else fill with cmn_str)\n",
    " - if next does not exist (meaning last index): fill it with previous value (only if previous is not '?', else fill with cmn_str)\n",
    " - if both exist: fill it with next value (only if next is not '?', else fill with previous value(only if it is not '?', else cmn_str))<br>  \n",
    " See the example provided below in comments for more details.\n",
    " \n",
    "*Hint: [Pandas series mode](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.mode.html#pandas.Series.mode) might be useful*\n",
    " \n",
    "**Note:** Due to the presence of '?', the attribute values are treated as string values. It's important to convert the values into numeric type for mean and min calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LJcKsWg2flW6",
    "jupyter": {
     "outputs_hidden": true
    },
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-9769d09e3c947c52",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "### 1 Mark\n",
    "def fillCATEGORICAL(column):\n",
    "    \"\"\"\n",
    "    Input: 'column' is a pandas series object representing an attribute column in the dataset\n",
    "    Output:  return the modified column (series object) after filling in the missing values with the above procedure\n",
    "    Approach: Get the indices of missing values (brute force is fine), then scan around those locations.\n",
    "              Eg. column is [?,?,alp,alp,mod,alp,beta,?,?,?,mod,alp,beta,alp,alp,alp,alp,?,?],\n",
    "                  alp is the most common string,\n",
    "                  output should be - [alp,alp,alp,alp,mod,alp,beta,beta,alp,mod,mod,alp,beta,alp,alp,alp,alp,alp,alp]\n",
    "    Note: Assume that the number of missing values is always less than the most common string\n",
    "    \"\"\"\n",
    "    # ind = []\n",
    "    # ABOVE VARIABLE IS NOT MANDATORY TO USE\n",
    "    \n",
    "    ### BEGIN SOLUTION\n",
    "    pattern = ['?']\n",
    "    \n",
    "    test_list = list(column)\n",
    "    res = [ind for ind, val in enumerate(test_list) if val in set(pattern)] \n",
    "    # find most common string\n",
    "    common_car = column.mode()[0]\n",
    "    \n",
    "    for i in res:\n",
    "        \n",
    "        if (i-1)>=0 and i+1<len(column):\n",
    "            if test_list[i-1]=='?' and test_list[i+1]=='?':\n",
    "               \n",
    "                column.iloc[i] = common_car\n",
    "            elif test_list[i-1]=='?' and test_list[i+1]!='?':\n",
    "                \n",
    "                column.iloc[i] = column.iloc[i+1]\n",
    "            elif test_list[i-1]!='?' and test_list[i+1]=='?':\n",
    "                \n",
    "                column.iloc[i] = column.iloc[i-1]\n",
    "                \n",
    "            else:\n",
    "                \n",
    "                column.iloc[i] = column.iloc[i+1]\n",
    "        elif (i-1)>=0:\n",
    "            if test_list[i-1]=='?':\n",
    "                column.iloc[i] = common_car\n",
    "            else:\n",
    "                column.iloc[i] = column.iloc[i-1]\n",
    "        else:\n",
    "            if test_list[i+1]=='?':\n",
    "                column.iloc[i] = common_car\n",
    "            else:\n",
    "                column.iloc[i] = column.iloc[i+1]\n",
    "                \n",
    "    ### END SOLUTION\n",
    "    return column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fbSMD8WNflXi",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-83807e1352cf49d5",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "outputId": "7b64ce9d-da96-4bad-cef7-9a3bd488fda8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running sample test case\n",
      "Sample test case passed\n"
     ]
    }
   ],
   "source": [
    "# do not change code here\n",
    "### SAMPLE TEST CASE\n",
    "print(\"Running sample test case\")\n",
    "import copy\n",
    "dataC = data.copy(deep=True)\n",
    "dataN = data.copy(deep=True)\n",
    "dataR = data.copy(deep=True)\n",
    "\n",
    "dataC['A1'] = fillCATEGORICAL(dataC['A1'])\n",
    "dataC['A4'] = fillCATEGORICAL(dataC['A4'])\n",
    "dataC['A5'] = fillCATEGORICAL(dataC['A5'])\n",
    "dataC['A6'] = fillCATEGORICAL(dataC['A6'])\n",
    "dataC['A7'] = fillCATEGORICAL(dataC['A7'])\n",
    "\n",
    "# assert dataC.equals(pd.read_pickle('categorical_credits.p'))\n",
    "pd.testing.assert_frame_equal(dataC, pd.read_pickle('categorical_credits.p'))\n",
    "print(\"Sample test case passed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jpspqmM6flXs",
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-3e5b0b1b930f3583",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# do not change code here\n",
    "# Hidden Test cases\n",
    "### BEGIN HIDDEN TESTS\n",
    "\n",
    "dx = pd.Series([\"?\",\"?\",\"Ny\",\"Jrt\",\"?\",\"Jkt\",\"Ny\",\"?\",\"?\",\"?\",\"snp\",\"kx\",\"kx\",\"kx\",\"kx\",\"kx\",\"kx\",\"kx\"])\n",
    "dc = pd.Series([\"kx\",\"Ny\",\"Ny\",\"Jrt\",\"Jkt\",\"Jkt\",\"Ny\",\"Ny\",\"kx\",\"snp\",\"snp\",\"kx\",\"kx\",\"kx\",\"kx\",\"kx\",\"kx\",\"kx\"])\n",
    "assert dc.equals(fillCATEGORICAL(dx))\n",
    "\n",
    "### END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3X1cX_NkflXM",
    "jupyter": {
     "outputs_hidden": true
    },
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-a1b134a8f17dade7",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "### 1 Mark\n",
    "def fillREAL(column):\n",
    "    \"\"\"\n",
    "    Input: 'column' is a pandas series object representing an attribute column in the dataset\n",
    "    Output: return the modified column (series object) after filling in the missing values with the mean of that column.\n",
    "            The mean should be calculated ignoring the presence of '?' and it should be rounded off to 2 decimal places\n",
    "            The return data type of the series should be float64\n",
    "    \"\"\"\n",
    "    # ind = []\n",
    "    # cumsum = 0\n",
    "    # count = 0\n",
    "    # ABOVE VARIABLES ARE NOT MANDATORY TO USE\n",
    "    \n",
    "    ### BEGIN SOLUTION\n",
    "    res = []\n",
    "    count=0\n",
    "    cumsum=0\n",
    "    for i in range(0,len(column)):\n",
    "        if column.iloc[i]=='?':\n",
    "            column.iloc[i]=float(0)\n",
    "            res.append(i)\n",
    "        else:\n",
    "            column.iloc[i] = float(column.iloc[i])\n",
    "            count +=1\n",
    "            cumsum +=float(column.iloc[i])\n",
    "    mean = round(float(cumsum/count),2)\n",
    "    for k in res:\n",
    "        column.iloc[k] = float(mean)\n",
    "        \n",
    "    return column.astype(np.float64)\n",
    "    ### END SOLUTION\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "apfHGTBsflXy",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-dc328990510da49c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "outputId": "bed5a656-4cc7-433c-9c3d-722bac543507"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running sample test case\n",
      "Sample test case passed\n"
     ]
    }
   ],
   "source": [
    "# do not change code here\n",
    "### SAMPLE TEST CASE\n",
    "print(\"Running sample test case\")\n",
    "dataR['A2'] = fillREAL(dataR['A2'])\n",
    "# assert dataR.equals(pd.read_pickle('real_credits.p'))\n",
    "pd.testing.assert_frame_equal(dataR, pd.read_pickle('real_credits.p'))\n",
    "print(\"Sample test case passed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "paDF5HD0flX4",
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-240333c4f2d9aea0",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# do not change code here\n",
    "# Hidden Test cases\n",
    "### BEGIN HIDDEN TESTS\n",
    "\n",
    "al = pd.Series([1,'?',3,4,'?',4,20,16,'?','?'])\n",
    "ans = pd.Series([1,8,3,4,8,4,20,16,8,8]).astype(np.float64)\n",
    "assert fillREAL(al).equals(ans)\n",
    "\n",
    "### END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OoUq2R0_flXY",
    "jupyter": {
     "outputs_hidden": true
    },
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-7c639ce793bb5ee3",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "### 0.75 Mark\n",
    "def fillINT(column):\n",
    "    \"\"\"\n",
    "    Input: 'column' is a pandas series object representing an attribute column in the dataset\n",
    "    Output: return the modified column (series object) after filling in the missing values with the minimum value \n",
    "            of that column. The return type should be int32\n",
    "    \"\"\"\n",
    "    # ind = []\n",
    "    # minVal = 100000000\n",
    "    # ABOVE VARIABLES ARE NOT MANDATORY TO USE\n",
    "    \n",
    "    ### BEGIN SOLUTION\n",
    "    res = []\n",
    "    minVal = 100000000\n",
    "    \n",
    "    for i in range(0,len(column)):\n",
    "        if column.iloc[i]=='?':\n",
    "            column.iloc[i]=0\n",
    "            res.append(i)\n",
    "        else:\n",
    "            column.iloc[i] = int(column.iloc[i])\n",
    "            if minVal > column.iloc[i]:\n",
    "                minVal = column.iloc[i]\n",
    "    for k in res:\n",
    "        column.iloc[k] = minVal\n",
    "                   \n",
    "    return column.astype(np.int32)\n",
    "    \n",
    "    ### END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Etvh8K89flX9",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-e16664c7edad976a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "outputId": "614627e9-9213-4b59-acbd-08cd1081744c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running sample test case\n",
      "Sample test case passed\n"
     ]
    }
   ],
   "source": [
    "# do not change code here\n",
    "### SAMPLE TEST CASE\n",
    "print(\"Running sample test case\")\n",
    "dataN['A10'] = fillINT(dataN['A10'])\n",
    "# assert dataN.equals(pd.read_pickle('numeric_credits.p'))\n",
    "pd.testing.assert_frame_equal(dataN, pd.read_pickle('numeric_credits.p'))\n",
    "print(\"Sample test case passed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Lc4ljDJoflYC",
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-97f5c7e63d972d90",
     "locked": true,
     "points": 0.75,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# do not change code here\n",
    "# Hidden Test cases\n",
    "### BEGIN HIDDEN TESTS\n",
    "\n",
    "al = pd.Series([44,'?',32,41,'?',14,20,16,'?','?',12])\n",
    "ans = pd.Series([44,12,32,41,12,14,20,16,12,12,12]).astype(np.int32)\n",
    "assert fillINT(al).equals(ans)\n",
    "\n",
    "### END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "f6RkICB7flYJ",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-f06f480c551f3dfc",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### 2.3  Final Preprocessing and Data split(1 Mark)\n",
    "\n",
    "**The tasks to be performed:**<br>\n",
    "\n",
    "- Encode the categorical attributes (0.25 Mark)\n",
    "- Scale the data to the range [0-1] (0.25 Mark)\n",
    "- Split the dataset into training and testing as per the given instructions (0.50 Mark) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-4df5e9c1b57332fa",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "##### Encode the categorical attributes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TJ7exK-3flYL",
    "jupyter": {
     "outputs_hidden": true
    },
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-67ac4746e399a890",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# do not change code here\n",
    "def labelEncode(col):\n",
    "    \"\"\"\n",
    "    function takes a series object (column of a dataframe) as an input and outputs an label encoded series object\n",
    "    \"\"\"\n",
    "    return LabelEncoder().fit_transform(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zJTanQoxflYP",
    "jupyter": {
     "outputs_hidden": true
    },
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-7e4e77c9cc18c67b",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "### 0.25 Mark\n",
    "def encode_dataset(dat):\n",
    "    \"\"\"\n",
    "    Input: dat is the dataframe whose attributes need to be encoded. Shape of dat - (no of instances, no of attr)\n",
    "    \n",
    "    Output: return the encoded dataframe with the same shape\n",
    "    \n",
    "    Approach: Loop through the attributes of dataframe. Check every attribute type - if its categorical/string encode that\n",
    "              column.\n",
    "    \"\"\"\n",
    "    ### BEGIN SOLUTION\n",
    "    for i in range(0,dat.shape[1]):\n",
    "        if type(dat.iloc[0,i])== str:\n",
    "            dat.iloc[:,i] = labelEncode(dat.iloc[:,i])\n",
    "    return dat\n",
    "    \n",
    "    ### END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iIHD4o-CflYW",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-571b526cc8a32f4b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "outputId": "41ff7aac-5cc8-4703-baa8-3f8c59c344b0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running sample test case\n",
      "Sample test case passed\n"
     ]
    }
   ],
   "source": [
    "# do not change code here\n",
    "### SAMPLE TEST CASE\n",
    "print(\"Running sample test case\")\n",
    "encoded_data = encode_dataset(dataC)\n",
    "assert encoded_data.equals(pd.read_pickle('categorical_encode.p'))\n",
    "print(\"Sample test case passed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bCf0YnQZflYb",
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-b24b69589255a0e9",
     "locked": true,
     "points": 0.25,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# do not change code here\n",
    "# Hidden Test cases\n",
    "### BEGIN HIDDEN TESTS\n",
    "\n",
    "dc = pd.Series([\"kx\",\"Ny\",\"Ny\",\"Jrt\",\"Jkt\",\"Jkt\",\"Ny\",\"Ny\",\"kx\",\"snp\",\"snp\",\"kx\",\"kx\",\"kx\",\"kx\",\"kx\",\"kx\",\"kx\"])\n",
    "dr = pd.Series([1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18])\n",
    "dz = pd.Series([0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1.0,1.1,1.2,1.3,1.4,1.5,1.6,1.7,1.8])\n",
    "dm = pd.Series([\"a\",\"aa\",\"b\",\"ab\",\"ba\",\"bb\",\"ab\",\"aa\",\"ba\",\"ab\",\"aa\",\"bb\",\"b\",\"b\",\"a\",\"ba\",\"a\",\"a\"])\n",
    "series = [dc,dr,dz,dm]\n",
    "synDf = pd.concat(series,axis=1)\n",
    "\n",
    "dcENC = pd.Series([3,2,2,1,0,0,2,2,3,4,4,3,3,3,3,3,3,3])\n",
    "dmENC = pd.Series([0,1,3,2,4,5,2,1,4,2,1,5,3,3,0,4,0,0]) \n",
    "seriesENC = [dcENC,dr,dz,dmENC]\n",
    "synENC = pd.concat(seriesENC,axis=1)\n",
    "\n",
    "assert synENC.equals(encode_dataset(synDf))\n",
    "\n",
    "### END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Kz6qelGVflYf",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-a4863570d913dac4",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "##### Assembling all the preprocessing steps\n",
    "\n",
    "Before moving onto scaling the dataset in the specified range, it is important that the data fed into scaler have all missing values filled and attributes encoded. The below preprocess() does the same. It calls all the operations performed above on the original dataset and produces a dataframe suitable to be fed into the scaler<br>\n",
    "*You are not required to make any changes/edits to this function*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Vw9rcEAGflYh",
    "jupyter": {
     "outputs_hidden": true
    },
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-38721bb259c85dad",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# do not change code here\n",
    "def preprocess(dat):\n",
    "    \n",
    "    dat2 = dat.copy(deep=True)\n",
    "    \n",
    "    # calling categorical fill for cols A1, A4, A5, A6, A7\n",
    "    catfill = ['A1','A4','A5','A6','A7']\n",
    "    for col in catfill:\n",
    "        dat2[col] = fillCATEGORICAL(dat2[col])\n",
    "    \n",
    "    # calling Real fill for cols A2\n",
    "    dat2['A2'] = fillREAL(dat2['A2'])\n",
    "    \n",
    "    # calling Int fill for col A10 \n",
    "    dat2['A10'] = fillINT(dat2['A10'])\n",
    "    \n",
    "    # encode the dataset\n",
    "    enc_data = encode_dataset(dat2)\n",
    "    \n",
    "    return enc_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-d107e6d3ec9df09e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "##### Scale the data to the range [0-1]:\n",
    "**Note:** Use [min-max scaler](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html) for scaling the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aU8BfQ95flYk",
    "jupyter": {
     "outputs_hidden": true
    },
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-e1954f1e24a4969b",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "### 0.25 Mark. \n",
    "def scaler(df,colname):\n",
    "    \"\"\"\n",
    "    Input: df is a dataframe of shape (number of instances,number of attributes)\n",
    "           colname is the list of attribute names of the dataframe\n",
    "    \n",
    "    Output: return a DataFrame, with the data being scaled in the range 0-1. The scaled data values should be rounded off\n",
    "            to 2 decimal places\n",
    "    \n",
    "    Approach: create a scaler object and apply it on the input, by calling its pre-defined function (fit_transform())\n",
    "    \"\"\"\n",
    "    ### BEGIN SOLUTION\n",
    "    sc = MinMaxScaler()\n",
    "    return round(pd.DataFrame(sc.fit_transform(df),columns=colname),2)\n",
    "    ### END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GdAak5DZflYq",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-f1481fdeb8eacd93",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "outputId": "dcda122e-c6a0-4ce4-997b-666d6295c2db"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running sample test case\n",
      "Sample test case passed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/parth/anaconda3/lib/python3.7/site-packages/sklearn/preprocessing/data.py:323: DataConversionWarning: Data with input dtype int32, int64, float64 were all converted to float64 by MinMaxScaler.\n",
      "  return self.partial_fit(X, y)\n"
     ]
    }
   ],
   "source": [
    "# do not change code here\n",
    "### SAMPLE TEST CASE\n",
    "print(\"Running sample test case\")\n",
    "processed_data = preprocess(data)\n",
    "colnames = ['A1','A2','A3','A4','A5','A6','A7','A8','A9','A10','A11','A12']\n",
    "scaled_data = scaler(processed_data,colnames)\n",
    "assert scaled_data.equals(pd.read_pickle('data_normalized.p'))\n",
    "print(\"Sample test case passed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "svcwe4CEflYv",
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-e9b2e6844b0e6546",
     "locked": true,
     "points": 0.25,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/parth/anaconda3/lib/python3.7/site-packages/sklearn/preprocessing/data.py:323: DataConversionWarning: Data with input dtype int64 were all converted to float64 by MinMaxScaler.\n",
      "  return self.partial_fit(X, y)\n"
     ]
    }
   ],
   "source": [
    "# do not change code here\n",
    "# Hidden Test cases\n",
    "### BEGIN HIDDEN TESTS\n",
    "\n",
    "dc = pd.Series([3,2,2,1,0,0,2])\n",
    "dr = pd.Series([1,2,3,4,5,6,7])\n",
    "dm = pd.Series([0,1,3,2,4,5,2])\n",
    "ser = [dc,dr,dm]\n",
    "syndf = pd.concat(ser,axis=1)\n",
    "res = scaler(syndf,[0,1,2])\n",
    "\n",
    "ser1 = pd.Series([1.00,0.67,0.67,0.33,0.00,0.00,0.67])\n",
    "ser2 = pd.Series([0.00,0.17,0.33,0.50,0.67,0.83,1.00])\n",
    "ser3 = pd.Series([0.0,0.2,0.6,0.4,0.8,1.0,0.4])\n",
    "\n",
    "scl = [ser1,ser2,ser3]\n",
    "dfscl = pd.concat(scl,axis=1)\n",
    "\n",
    "assert dfscl.equals(round(res,2))\n",
    "\n",
    "### END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rQ0rSpwGflYz",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-1724ac9521f861a4",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "##### Train and Test split:\n",
    "\n",
    "The dataset needs to be split into testing and training data. We won't perform any random subsampling, instead divide the dataset into different groups and merge them into TRAIN AND TEST SET<br>\n",
    "\n",
    "**Divide the dataset into 4 groups:**<br>\n",
    "Group 1 LOWLIMIT[0] - HILIMIT[0] <br>\n",
    "Group 2 LOWLIMIT[1] - HILIMIT[1] <br>\n",
    "Group 3 LOWLIMIT[2] - HILIMIT[2] <br>\n",
    "Group 4 LOWLIMIT[3] - HILIMIT[3] <br>\n",
    "\n",
    "LOWLIMIT and HILIMIT are 2 lists which store the row/instance number of the dataset. <br>\n",
    "If HILIMIT:[99, 399, 599, 690], LOLIMIT:[0, 100, 400, 600]. <br>\n",
    "    Group 1 will be formed from rows starting from 0 till 99 (i.e. from 0th row to 99th row of dataframe)<br>\n",
    "    Group 2 will be formed from rows starting from 100 till 399 and so on <br>\n",
    "\n",
    "**After the division, create the train and test set in the following way:**\n",
    " - Train dataset - Stack 1st and 3rd group\n",
    " - Test dataset - Stack 2nd and 4th group<br><br>\n",
    "**Note:** The last column contains the labels of the dataset <br>\n",
    "Hint: Few functions which might be useful: [pandas concat](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.concat.html), [pandas to numpy (for pandas version >= 0.24.0)](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.to_numpy.html) / [pandas values (for pandas version < 0.24.0)](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.values.html) and [numpy transpose](https://docs.scipy.org/doc/numpy/reference/generated/numpy.transpose.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RwXlTerXflYz",
    "jupyter": {
     "outputs_hidden": true
    },
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-ecb4d1bf3524961a",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "### 0.5 Mark\n",
    "def train_and_test_split(data,lolimit,hilimit):\n",
    "    \"\"\"\n",
    "    Complete the function using the logic described above\n",
    "    \n",
    "    Input: input 'df' is a dataframe of shape (no of instances, no of attr) representing the dataset\n",
    "    \n",
    "    Output: Output 4 numpy 2D arrays trainX, trainY, testX, testY of shape:\n",
    "            trainX,testX : (no of attr/features, no of instances)\n",
    "            trainY,testY: (1, no of instances)\n",
    "            \n",
    "    Approach: Divide the dataset into 4 groups mentioned[create temporary dataframes]. Concatenate accordingly.  \n",
    "              Now convert the dataframes into numpy 2D arrays. After the conversion,transpose the arrays so as to match the \n",
    "              the required shape of returned variables\n",
    "    \"\"\"\n",
    "    ### BEGIN SOLUTION\n",
    "    \n",
    "    datatemp1 = data[lolimit[0]:hilimit[0]+1]\n",
    "    datatemp2 = data[lolimit[1]:hilimit[1]+1]\n",
    "    datatemp3 = data[lolimit[2]:hilimit[2]+1]\n",
    "    datatemp4 = data[lolimit[3]:hilimit[3]+1]\n",
    "    \n",
    "    trainX = pd.concat([datatemp1.iloc[:,:-1],datatemp3.iloc[:,:-1]], ignore_index=True)\n",
    "    trainY = pd.concat([datatemp1.iloc[:,-1:],datatemp3.iloc[:,-1:]], ignore_index=True)\n",
    "    testX = pd.concat([datatemp2.iloc[:,:-1],datatemp4.iloc[:,:-1]], ignore_index=True)\n",
    "    testY = pd.concat([datatemp2.iloc[:,-1:],datatemp4.iloc[:,-1:]], ignore_index=True)\n",
    "    \n",
    "    trainX = trainX.values\n",
    "    trainX = np.transpose(trainX)#.reshape(trainX.shape[1],trainX.shape[0])\n",
    "    trainY = trainY.values\n",
    "    trainY = np.transpose(trainY)#.reshape(trainY.shape[1],trainY.shape[0])\n",
    "    testX = testX.values\n",
    "    testX = np.transpose(testX)#.reshape(testX.shape[1],testX.shape[0])\n",
    "    testY = testY.values\n",
    "    testY = np.transpose(testY)#.reshape(testY.shape[1],testY.shape[0])\n",
    "    \n",
    "    return trainX,trainY,testX,testY\n",
    "    \n",
    "    ### END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "a2UEG_OKflY7",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-1d1939b5fa089eb5",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running sample test case\n",
      "Sample test case passed\n"
     ]
    }
   ],
   "source": [
    "# do not change code here\n",
    "### SAMPLE TEST CASE\n",
    "print(\"Running sample test case\")\n",
    "LOLIMIT = [0,100,400,600]\n",
    "HILIMIT = [99,399,599,690]\n",
    "X_train, Y_train, X_test, Y_test = train_and_test_split(scaled_data,LOLIMIT,HILIMIT)\n",
    "assert np.array_equal(X_train,np.load('trainX.npy'))\n",
    "assert np.array_equal(Y_train,np.load('trainY.npy'))\n",
    "assert np.array_equal(X_test,np.load('testX.npy'))\n",
    "assert np.array_equal(Y_test,np.load('testY.npy'))\n",
    "print(\"Sample test case passed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-a545f20e1add5884",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11, 300)\n",
      "(1, 300)\n",
      "(11, 390)\n",
      "(1, 390)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(Y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CxPg2k0yflY-",
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-605f02ca0f7aec29",
     "locked": true,
     "points": 0.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# do not change code here\n",
    "# Hidden Test cases\n",
    "### BEGIN HIDDEN TESTS\n",
    "\n",
    "dat = np.array([[1, 4, 4],\n",
    "       [3, 1, 4],\n",
    "       [1, 1, 3],\n",
    "       [1, 1, 3],\n",
    "       [3, 4, 1],\n",
    "       [1, 3, 2],\n",
    "       [3, 3, 1],\n",
    "       [3, 1, 2]])\n",
    "\n",
    "dx = pd.DataFrame(dat)\n",
    "res1 = np.array([[1, 3, 3, 1],\n",
    "       [4, 1, 4, 3]])\n",
    "res2 = np.array([[4, 4, 1, 2]])\n",
    "res3 = np.array([[1, 1, 3, 3],\n",
    "       [1, 1, 3, 1]])\n",
    "res4 = np.array([[3, 3, 1, 2]])\n",
    "llim = [0,2,4,6]\n",
    "hlim = [1,3,5,7]\n",
    "tr1,tr2,tr3,tr4 = train_and_test_split(dx,llim,hlim)\n",
    "\n",
    "assert np.array_equal(res1,tr1)\n",
    "assert np.array_equal(res2,tr2)\n",
    "assert np.array_equal(res3,tr3)\n",
    "assert np.array_equal(res4,tr4)\n",
    "\n",
    "### END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "66NBJBQY3qQ4",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-b3b1a74cfffa8fb0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### 3- Activation Functions (2 Marks):\n",
    "\n",
    "Complete the below functions, wherein each Python function corresponds to an activation function.  The equations for various activation functions are: \n",
    "\n",
    "$$linear(z) = z$$\n",
    "$$relu(z) = max(0, z)$$\n",
    "$$sigmoid(z) = 1 / (1 + e^{-z})$$\n",
    "$$tanh(z) = 2/(1+e^{-2z})-1$$\n",
    "$$leaky\\_relu(z) = max(\\alpha*z, z)$$\n",
    "$$softplus(z) = ln(1+e^{z})$$\n",
    "$$ ELU(z)=   \\left\\{\n",
    "\\begin{array}{ll}\n",
    "      z & z>=0 \\\\\n",
    "      \\alpha*(e^{z}-1) & z < 0 \\\\\n",
    "\\end{array} \n",
    "\\right.  $$:\n",
    "\n",
    "The plots of the above activation functions are:\n",
    "\n",
    "<centre><img src=\"index.png\" height=\"500\" width=\"500\"></centre>\n",
    "\n",
    "Hints: \n",
    "* You may find NumPy Masking an useful concept while implementing some of the below functions (<a href=\"https://jakevdp.github.io/PythonDataScienceHandbook/02.06-boolean-arrays-and-masks.html\">NumPy Masking</a>).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nDzsU6D568cU",
    "jupyter": {
     "outputs_hidden": true
    },
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-ba1b11dafe77a143",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "### 0.25 Marks\n",
    "def linear(z):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "        z : z can either be a number or a numpy array\n",
    "    Outputs:\n",
    "        Linear function applied to z; if z is an array, the function must be applied to its each element\n",
    "    \"\"\"\n",
    "    ### BEGIN SOLUTION\n",
    "    return z\n",
    "    ### END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-13e048a065c1a6c6",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running sample test case\n",
      "Sample test case passed\n"
     ]
    }
   ],
   "source": [
    "# do not change code here\n",
    "### SAMPLE TEST CASE\n",
    "print(\"Running sample test case\")\n",
    "z = np.array([[-1.2, 2.0]])\n",
    "l_z = np.array([[-1.2,  2. ]])\n",
    "assert np.allclose(l_z, linear(z), atol = 1e-5)\n",
    "print(\"Sample test case passed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-f9de8f343c48831e",
     "locked": true,
     "points": 0.25,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# do not change code here\n",
    "# Hidden Test cases\n",
    "### BEGIN HIDDEN TESTS\n",
    "\n",
    "z = np.array([[-1.2, 8.5], [0.0, -9.5]])\n",
    "l_z = np.array([[-1.2,  8.5], [ 0.,  -9.5]])\n",
    "assert linear(z).shape == (2, 2)\n",
    "assert np.allclose(l_z, linear(z), atol = 1e-5)\n",
    "\n",
    "# print(linear(z))\n",
    "\n",
    "### END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VdXx963u3swq",
    "jupyter": {
     "outputs_hidden": true
    },
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-7b400295b1d02aab",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "### 0.25 Marks\n",
    "def relu(z):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "        z : z can either be a number or a numpy array\n",
    "    Outputs:\n",
    "        ReLU function applied to z; if z is an array, the function must be applied to its each element\n",
    "    \"\"\"\n",
    "    ### BEGIN SOLUTION\n",
    "    return np.maximum(0, z)   \n",
    "    ### END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-891fbf2d37f1ce0d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running sample test case\n",
      "Sample test case passed\n"
     ]
    }
   ],
   "source": [
    "# do not change code here\n",
    "### SAMPLE TEST CASE\n",
    "print(\"Running sample test case\")\n",
    "z = np.array([[-1.2, 2.0]])\n",
    "r_z = np.array([[0., 2.]])\n",
    "assert np.allclose(r_z, relu(z), atol = 1e-5)\n",
    "print(\"Sample test case passed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-ad4fd6f3d828ec63",
     "locked": true,
     "points": 0.25,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# do not change code here\n",
    "# Hidden Test cases\n",
    "### BEGIN HIDDEN TESTS\n",
    "\n",
    "z = np.array([[-1.2, 8.5], [0.0, -9.5]])\n",
    "r_z = np.array([[0.,  8.5], [0.,  0. ]])\n",
    "assert relu(z).shape == (2, 2)\n",
    "assert np.allclose(r_z, relu(z), atol = 1e-5)\n",
    "\n",
    "# print(relu(z))\n",
    "\n",
    "### END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DohE_X8-3zwG",
    "jupyter": {
     "outputs_hidden": true
    },
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-89c0640996913055",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "### 0.25 Marks\n",
    "def sigmoid(z):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "        z : z can either be a number or a numpy array\n",
    "    Outputs:\n",
    "        Sigmoid function applied to z; if z is an array, the function must be applied to its each element\n",
    "    \"\"\"\n",
    "    ### BEGIN SOLUTION\n",
    "    return 1/(1+np.exp(-z))\n",
    "    ### END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-9cdd51c68ec54b2e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running sample test case\n",
      "Sample test case passed\n"
     ]
    }
   ],
   "source": [
    "# do not change code here\n",
    "### SAMPLE TEST CASE\n",
    "print(\"Running sample test case\")\n",
    "z = np.array([[-1.2, 2.0]])\n",
    "s_z = np.array([[0.23147522, 0.88079708]])\n",
    "assert np.allclose(s_z, sigmoid(z), atol = 1e-5)\n",
    "print(\"Sample test case passed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-bd8753f0eb84a683",
     "locked": true,
     "points": 0.25,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# do not change code here\n",
    "# Hidden Test cases\n",
    "### BEGIN HIDDEN TESTS\n",
    "\n",
    "z = np.array([[-1.2, 8.5], [0.0, -9.5]])\n",
    "s_z = np.array([[2.31475217e-01, 9.99796573e-01], [5.00000000e-01, 7.48462275e-05]])\n",
    "assert sigmoid(z).shape == (2, 2)\n",
    "assert np.allclose(s_z, sigmoid(z), atol = 1e-5)\n",
    "\n",
    "# print(sigmoid(z))\n",
    "\n",
    "### END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "No8Zo23FR6Xx",
    "jupyter": {
     "outputs_hidden": true
    },
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-427ecb3fd8aa6923",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "### 0.25 Marks\n",
    "def tanh(z):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "        z : z can either be a number or a numpy array\n",
    "    Outputs:\n",
    "        Tanh function applied to z; if z is an array, the function must be applied to its each element\n",
    "    \"\"\"\n",
    "    ### BEGIN SOLUTION\n",
    "    return (2/(1+np.exp(-2*z))-1)\n",
    "    ### END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-358b265d01633ac6",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running sample test case\n",
      "Sample test case passed\n"
     ]
    }
   ],
   "source": [
    "# do not change code here\n",
    "### SAMPLE TEST CASE\n",
    "print(\"Running sample test case\")\n",
    "z = np.array([[-1.2, 2.0]])\n",
    "t_z = np.array([[-0.83365461,  0.96402758]])\n",
    "assert np.allclose(t_z, tanh(z), atol = 1e-5)\n",
    "print(\"Sample test case passed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-52badad0eb7702ca",
     "locked": true,
     "points": 0.25,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# do not change code here\n",
    "# Hidden Test cases\n",
    "### BEGIN HIDDEN TESTS\n",
    "z = np.array([[-1.2, 8.5], [0.0, -9.5]])\n",
    "t_z = np.array([[-0.83365461,  0.99999992], [ 0., -0.99999999]])\n",
    "assert tanh(z).shape == (2, 2)\n",
    "assert np.allclose(t_z, tanh(z), atol = 1e-5)\n",
    "\n",
    "# print(tanh(z))\n",
    "\n",
    "### END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lTDIz1vXShpi",
    "jupyter": {
     "outputs_hidden": true
    },
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-fd3ccaf5ddd37d72",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "### 0.25 Marks\n",
    "def leaky_relu(z, alpha=0.1):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "        z : z can either be a number or a numpy array\n",
    "        alpha - A hyperparameter\n",
    "    Outputs:\n",
    "        Leaky ReLU function applied to z; if z is an array, the function must be applied to its each element\n",
    "    \"\"\"\n",
    "    ### BEGIN SOLUTION\n",
    "    return np.maximum(alpha*z, z)\n",
    "    ### END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-9e21dffde75b6127",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running sample test case\n",
      "Sample test case passed\n"
     ]
    }
   ],
   "source": [
    "# do not change code here\n",
    "### SAMPLE TEST CASE\n",
    "print(\"Running sample test case\")\n",
    "z = np.array([[-1.2, 2.0]])\n",
    "lr_z = np.array([[-0.12,  2.  ]])\n",
    "assert np.allclose(lr_z, leaky_relu(z), atol = 1e-5)\n",
    "print(\"Sample test case passed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-602516c3c4311541",
     "locked": true,
     "points": 0.25,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# do not change code here\n",
    "# Hidden Test cases\n",
    "### BEGIN HIDDEN TESTS\n",
    "z = np.array([[-1.2, 8.5], [0.0, -9.5]])\n",
    "lr_z = np.array([[-0.12,  8.5 ], [ 0., -0.95]])\n",
    "assert leaky_relu(z).shape == (2, 2)\n",
    "assert np.allclose(lr_z, leaky_relu(z), atol = 1e-5)\n",
    "\n",
    "# print(leaky_relu(z))\n",
    "\n",
    "### END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2pV1EmAWTBaI",
    "jupyter": {
     "outputs_hidden": true
    },
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-6d90ee90a1f4a889",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "### 0.25 Marks\n",
    "def softplus(z):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "        z : z can either be a number or a numpy array\n",
    "    Outputs:\n",
    "        Softplus function applied to z; if z is an array, the function must be applied to its each element\n",
    "    \"\"\"\n",
    "    ### BEGIN SOLUTION\n",
    "    return np.log(1+np.exp(z))\n",
    "    ### END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-433ac71990b48ea4",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running sample test case\n",
      "Sample test case passed\n"
     ]
    }
   ],
   "source": [
    "# do not change code here\n",
    "### SAMPLE TEST CASE\n",
    "print(\"Running sample test case\")\n",
    "z = np.array([[-1.2, 2.0]])\n",
    "st_z = np.array([[0.26328247, 2.12692801]])\n",
    "assert np.allclose(st_z, softplus(z), atol = 1e-5)\n",
    "print(\"Sample test case passed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-90d8f87a61a171b4",
     "locked": true,
     "points": 0.25,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# do not change code here\n",
    "# Hidden Test cases\n",
    "### BEGIN HIDDEN TESTS\n",
    "z = np.array([[-1.2, 8.5], [0.0, -9.5]])\n",
    "st_z = np.array([[2.63282467e-01, 8.50020345e+00], [6.93147181e-01, 7.48490286e-05]])\n",
    "assert softplus(z).shape == (2, 2)\n",
    "assert np.allclose(st_z, softplus(z), atol = 1e-5)\n",
    "\n",
    "# print(softplus(z))\n",
    "\n",
    "### END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0_ln20_BTTjd",
    "jupyter": {
     "outputs_hidden": true
    },
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-9014616c5df3a769",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "### 0.50 Marks\n",
    "def ELU(z, alpha=0.1):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "        z : z can either be a number or a numpy array\n",
    "        alpha - A hyperparameter\n",
    "    Outputs:\n",
    "        Exponential Linear Unit function applied to z; if z is an array, the function must be applied to its each element\n",
    "    \"\"\"\n",
    "    ### BEGIN SOLUTION\n",
    "    a = np.array(z, copy=True)\n",
    "    neg_indices = z < 0\n",
    "    a[neg_indices] = alpha * (np.exp(z[neg_indices])-1)\n",
    "    return a\n",
    "    ### END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-e73fc5b4fd81172a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running sample test case\n",
      "Sample test case passed\n"
     ]
    }
   ],
   "source": [
    "# do not change code here\n",
    "### SAMPLE TEST CASE\n",
    "print(\"Running sample test case\")\n",
    "z = np.array([[-1.2, 2.0]])\n",
    "e_z = np.array([[-0.06988058,  2.]])\n",
    "assert np.allclose(e_z, ELU(z), atol = 1e-5)\n",
    "print(\"Sample test case passed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-55d36e5d78a3ff5c",
     "locked": true,
     "points": 0.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# do not change code here\n",
    "# Hidden Test cases\n",
    "### BEGIN HIDDEN TESTS\n",
    "\n",
    "z = np.array([[-1.2, 8.5], [0.0, -9.5]])\n",
    "e_z = np.array([[-0.06988058,  8.5], [ 0., -0.09999251]])\n",
    "assert ELU(z).shape == (2, 2)\n",
    "assert np.allclose(e_z, ELU(z), atol = 1e-5)\n",
    "\n",
    "# print(ELU(z))\n",
    "\n",
    "### END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "V5Ra7A4ep1Hi",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-141275b2072477f7",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### 4- Initialize Weights of the Neural Network/ Construct H-layer Neural Network (0.5 Marks):\n",
    "\n",
    "*   The NN will contain $H-1$ hidden layers and an output layer. For a layer $h$, we will have two parameters/variables:\n",
    "  * Weight matrix $W^{[h]}$(of shape (size of current layer, size of previous layer)),\n",
    "  * Bias vector $b^{[h]}$(of shape (size of current layer, 1)),  \n",
    "\n",
    "*   Use np.random.uniform(low, high, shape), with low=0.0 and high=0.01 to initialize the weights $W^{[h]}$ (<a href=\"https://docs.scipy.org/doc/numpy-1.15.0/reference/generated/numpy.random.uniform.html#numpy-random-uniform\">np.random.uniform()</a>).\n",
    "\n",
    "*   Initialize biases $b^{[h]}$ with zeros (<a href=\"https://docs.scipy.org/doc/numpy/reference/generated/numpy.zeros.html\">np.zeros()</a>).\n",
    "\n",
    "* For a single-hidden layer NN with input of size 5, hidden layer size = 4 and output layer size =3, the input parameter ```layers_sizes``` will be **[5, 4, 3]**. The output of the ```construct_NN()``` function will be  **```[[np.array(shape=(4, 5)), np.array(shape=(4, 1))], [np.array(shape=(3, 4)), np.array(shape=(3, 1))]]```**, with each numpy array initialised as mentioned above.\n",
    "\n",
    "\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XnhdOy-ZnNks",
    "jupyter": {
     "outputs_hidden": true
    },
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-c1bf86aa64085f2d",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "### 0.50 Marks\n",
    "def construct_NN(layers_sizes):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "        layers_sizes - A list (of length H+1) containing the size (number of neurons) of each layer. Element at index 0 represents input layer size.  \n",
    "    Outputs:\n",
    "        weights - A list (of length H), where each element is a list - [Wh, bh], representing the  weights matrix and bias vector of layer h (Both Wh and bh are numpy arrays).\n",
    "    \"\"\"\n",
    "    H = len(layers_sizes)-1\n",
    "    weights = []\n",
    "    for h in range(H):\n",
    "        # Append weights of h-th layer to \"weights\" list in the manner specified above.\n",
    "        ### BEGIN SOLUTION\n",
    "        weights.append(list([np.random.uniform(low=0.0, high = 0.01, size=(layers_sizes[h+1], layers_sizes[h])), np.zeros((layers_sizes[h+1], 1))]))\n",
    "        ### END SOLUTION\n",
    "    return weights "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "FOjXEqd6JPwX",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-58780b28f3f6c0e7",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "outputId": "b9ae5c3e-7fd5-4016-bba9-97b9c6c559e7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running sample test case\n",
      "Sample test case passed\n"
     ]
    }
   ],
   "source": [
    "# do not change code here\n",
    "### SAMPLE TEST CASE\n",
    "print(\"Running sample test case\")\n",
    "np.random.seed(1)\n",
    "weights = construct_NN([4, 2, 1])\n",
    "assert np.allclose(weights[0][0], np.array([[4.17022005e-03, 7.20324493e-03, 1.14374817e-06, 3.02332573e-03],\n",
    " [1.46755891e-03, 9.23385948e-04, 1.86260211e-03, 3.45560727e-03]]), atol = 1e-5)\n",
    "assert np.allclose(weights[0][1], np.array([[0.], [0.]]), atol = 1e-5)\n",
    "assert np.allclose(weights[1][0], np.array([[0.00396767, 0.00538817]]), atol = 1e-5)\n",
    "assert np.allclose(weights[1][1], np.array([[0.]]), atol = 1e-5)\n",
    "print(\"Sample test case passed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wuVFpc6TJPlb",
    "jupyter": {
     "outputs_hidden": true
    },
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-92e465e4bbf4748d",
     "locked": true,
     "points": 0.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# do not change code here\n",
    "# Hidden Test cases\n",
    "### BEGIN HIDDEN TESTS\n",
    "\n",
    "np.random.seed(21)\n",
    "weights = construct_NN([4, 1])\n",
    "assert weights[0][0].shape == (1, 4)\n",
    "assert weights[0][1].shape == (1, 1)\n",
    "assert np.allclose(weights[0][0], np.array([[0.00048725, 0.0028911,  0.00720966, 0.00021616]]), atol = 1e-5)\n",
    "assert np.allclose(weights[0][1], np.array([[0.]]), atol = 1e-5)\n",
    "\n",
    "np.random.seed(221)\n",
    "weights = construct_NN([5, 4, 3, 2])\n",
    "assert weights[0][0].shape == (4, 5)\n",
    "assert weights[0][1].shape == (4, 1)\n",
    "assert weights[1][0].shape == (3, 4)\n",
    "assert weights[1][1].shape == (3, 1)\n",
    "assert weights[2][0].shape == (2, 3)\n",
    "assert weights[2][1].shape == (2, 1)\n",
    "assert np.allclose(weights[0][0], np.array([[0.00546571, 0.00964307, 0.00063893, 0.00383576, 0.00968025],\n",
    " [0.00120433, 0.0065449,  0.00737589, 0.00603502, 0.00087712],\n",
    " [0.00356939, 0.00403174, 0.00596547, 0.00161565, 0.00329716],\n",
    " [0.00106589, 0.00733555, 0.00729769, 0.00059208, 0.00027165]]), atol = 1e-5)\n",
    "assert np.allclose(weights[0][1], np.array([[0.], [0.], [0.], [0.]]), atol = 1e-5)\n",
    "assert np.allclose(weights[1][0], np.array([[0.00588794, 0.00307833, 0.00938998, 0.00163332],\n",
    " [0.00020756, 0.00627462, 0.00886823, 0.00938493],\n",
    " [0.00631009, 0.00877756, 0.00219373, 0.00602732]]), atol = 1e-5)\n",
    "assert np.allclose(weights[1][1], np.array([[0.], [0.], [0.]]), atol = 1e-5)\n",
    "assert np.allclose(weights[2][0], np.array([[0.0099916, 0.00757922, 0.00016561], [0.00391762, 0.00854038, 0.00707902]]), atol = 1e-5)\n",
    "assert np.allclose(weights[2][1], np.array([[0.], [0.]]), atol = 1e-5)\n",
    "\n",
    "# print(weights[0][0])\n",
    "# print(weights[0][1])\n",
    "# print(weights[1][0])\n",
    "# print(weights[1][1])\n",
    "# print(weights[2][0])\n",
    "# print(weights[2][1])\n",
    "\n",
    "### END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mKoBKxqg2AJH",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-d151e957cc348e09",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### 5- Forward Propagation (1 Mark):\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oRUyxFL5Hqan",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-4c851af8783f8b4c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### 5.1- Forward Propagation Step (0.25 Marks):\n",
    "This function carries out computations of a single layer of the neural network:\n",
    " $$ Z^{[h]} = W^{[h]}.A^{[h-1]}+b^{[h]}$$\n",
    " $$ A^{[h]} = g^{[h]}(Z^{[h]}) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "o67n7ba42IFp",
    "jupyter": {
     "outputs_hidden": true
    },
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-f06eca0a9520b68c",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "### 0.25 Marks\n",
    "def forward_step(A_prev, W, b, activation, alpha=0.1):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "        A_prev - Input data OR Previous layer's activations of shape (size of previous layer, number of examples)\n",
    "        W - Weight matrix of shape (size of current layer, size of previous layer)\n",
    "        b - Bias vector of shape (size of the current layer, 1)\n",
    "        activation - Python string denoting the activation function to be used in the current layer \n",
    "        alpha - A hyperparameter for activation functions leaky_relu and ELU.\n",
    "    Outputs :\n",
    "        A - Current layer's activations of shape (size of current layer, number of examples)\n",
    "        Z - Current layer's pre activation value, i.e. input of activation function\n",
    "    \"\"\"\n",
    "    # Compute Z as defined above.\n",
    "    ### BEGIN SOLUTION\n",
    "    Z = np.matmul(W, A_prev) + b\n",
    "    ### END SOLUTION\n",
    "    if activation == \"relu\":\n",
    "        A = relu(Z)\n",
    "    elif activation == \"sigmoid\":\n",
    "        A = sigmoid(Z)\n",
    "    elif activation == \"tanh\":\n",
    "        A = tanh(Z)\n",
    "    elif activation == \"leaky_relu\":\n",
    "        A = leaky_relu(Z, alpha)\n",
    "    elif activation == \"softplus\":\n",
    "        A = softplus(Z)\n",
    "    elif activation == \"ELU\":\n",
    "        A = ELU(Z, alpha)\n",
    "    else: # Linear activation\n",
    "        A = linear(Z)\n",
    "    return A, Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "uGo3shOoOpAY",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-86031c5eb47c7583",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "outputId": "058794ae-ae28-4b1d-ab90-ebb7a7f44434"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running sample test case\n",
      "Sample test case passed\n"
     ]
    }
   ],
   "source": [
    "# do not change code here\n",
    "### SAMPLE TEST CASE\n",
    "print(\"Running sample test case\")\n",
    "np.random.seed(0)\n",
    "A_prev = np.random.randn(3,2)\n",
    "W = np.random.randn(1,3)\n",
    "b = np.random.randn(1,1)\n",
    "_, Z = forward_step(A_prev, W, b, \"linear\")\n",
    "assert np.allclose(Z, np.array([[1.74569796, 0.55248139]]), atol=1e-5)\n",
    "print(\"Sample test case passed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fJcUYINIOo4G",
    "jupyter": {
     "outputs_hidden": true
    },
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-27c4ec252caf4119",
     "locked": true,
     "points": 0.25,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# do not change code here\n",
    "# Hidden Test cases\n",
    "### BEGIN HIDDEN TESTS\n",
    "\n",
    "np.random.seed(2)\n",
    "A_prev = np.random.randn(4,3)\n",
    "W = np.random.randn(2,4)\n",
    "b = np.random.randn(2,1)\n",
    "_, Z = forward_step(A_prev, W, b, \"linear\")\n",
    "assert Z.shape == (2, 3)\n",
    "assert np.allclose(Z, np.array([[-1.91612412,  0.12444451, -1.96265337], [1.39456439, -1.32635496, -0.29272238]]), atol=1e-5)\n",
    "\n",
    "### END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yuppHoprLU3z",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-7df9441085963ea4",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### 5.2- Forward Propagation Module (0.75 Marks):\n",
    "* This function iterates over the $H$ layers of the neural network using a for loop, computing the final output of the neural network. \n",
    "* It also outputs a ```layers_cache``` list (of length H), whose each element corresponds to a layer $h$ and is a list[$A^{[h-1]}, W^{[h]}, Z^{[h]}$]. Storing these values now will be helpful while computing gradients n the backward pass.\n",
    "* For the sake of clarity, consider a single-hidden layer NN with input of size 5, hidden layer size = 4 and output layer size =3. For this example, the output ```layers_cache``` list will be [[$X or A^{[0]}$, $W^{[1]}$, $Z^{[1]}$], [$A^{[1]}$, $W^{[2]}$, $Z^{[2]}$]]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "McFyqZKUBvVH",
    "jupyter": {
     "outputs_hidden": true
    },
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-959d37cab861887e",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "### 0.75 Marks\n",
    "def forward_module(X, weights, activations, alpha=0.1):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "        X - Input data of shape (size of input layer, number of examples)\n",
    "        weights - A list (of length H), where each element is a list - [Wh, bh], representing the weights matrix and bias vector of layer h (Both Wh and bh are numpy arrays).\n",
    "        activations - A list (of length H), where each element is a Python string representing the activation function to be used in the corresponding layer\n",
    "        alpha - A hyperparameter for activation functions leaky_relu and ELU.\n",
    "    Outputs:\n",
    "        AH - Output layer's activations\n",
    "        layers_cache - A list (of length H), where each element is a list [A_prev, W, Z], representing the values A_prev (input to the layer), W (Weight matrix of the layer)\n",
    "            and Z (pre-activation value) for layer h. The purpose of cache is to store values that will be needed during backpropagation.\n",
    "    \"\"\"\n",
    "    layers_cache = []\n",
    "    H = len(weights)\n",
    "    A = X\n",
    "    for h in range(H):\n",
    "        # Hint : Call forward_step() with appropriate input parameters. Also append to layers_cache.\n",
    "        ### BEGIN SOLUTION\n",
    "        A_prev = A\n",
    "        A, Z = forward_step(A_prev, weights[h][0], weights[h][1], activations[h], alpha)\n",
    "        layers_cache.append(list([A_prev, weights[h][0], Z]))\n",
    "        ### END SOLUTION\n",
    "    \n",
    "    return A, layers_cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "UPTz4NkYR4F1",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-e75e8eb33c4fdb0e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "outputId": "ac294d56-1989-40ca-ec03-9be233ee6e57"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running sample test case\n",
      "Sample test case passed\n"
     ]
    }
   ],
   "source": [
    "# do not change code here\n",
    "### SAMPLE TEST CASE\n",
    "print(\"Running sample test case\")\n",
    "np.random.seed(2)\n",
    "X = np.random.randn(3,2)\n",
    "W1 = np.random.randn(2,3)\n",
    "b1 = np.random.randn(2,1)\n",
    "W2 = np.random.randn(1,2)\n",
    "b2 = np.random.randn(1,1)\n",
    "weights = [[W1, b1], [W2, b2]]\n",
    "activations = [\"relu\", \"sigmoid\"]\n",
    "A, layers_cache = forward_module(X, weights, activations)\n",
    "assert np.allclose(A, np.array([[0.91270109, 0.49521752]]), atol=1e-5)\n",
    "assert np.allclose(layers_cache[0][0], np.array([[-0.41675785, -0.05626683],\n",
    "       [-2.1361961 ,  1.64027081],\n",
    "       [-1.79343559, -0.84174737]]), atol=1e-5)\n",
    "assert np.allclose(layers_cache[0][1], np.array([[ 0.50288142, -1.24528809, -1.05795222],\n",
    "       [-0.90900761,  0.55145404,  2.29220801]]), atol=1e-5)\n",
    "assert np.allclose(layers_cache[0][2], np.array([[ 4.38950832, -1.13883735],\n",
    "       [-6.02803078, -2.09170456]]), atol=1e-5)\n",
    "assert np.allclose(layers_cache[1][0], np.array([[4.38950832, 0.        ],\n",
    "       [0.        , 0.        ]]), atol=1e-5)\n",
    "assert np.allclose(layers_cache[1][1], np.array([[ 0.53905832, -0.5961597 ]]), atol=1e-5)\n",
    "assert np.allclose(layers_cache[1][2], np.array([[ 2.34707049, -0.0191305 ]]), atol=1e-5)\n",
    "print(\"Sample test case passed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DToX-y57R37A",
    "jupyter": {
     "outputs_hidden": true
    },
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-289f5f3b38ca96cc",
     "locked": true,
     "points": 0.75,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# do not change code here\n",
    "# Hidden Test cases\n",
    "### BEGIN HIDDEN TESTS\n",
    "\n",
    "np.random.seed(20)\n",
    "X = np.random.randn(4,2)\n",
    "W1 = np.random.randn(3,4)\n",
    "b1 = np.random.randn(3,1)\n",
    "W2 = np.random.randn(2,3)\n",
    "b2 = np.random.randn(2,1)\n",
    "weights = [[W1, b1], [W2, b2]]\n",
    "activations = [\"ELU\", \"tanh\"]\n",
    "A, layers_cache = forward_module(X, weights, activations)\n",
    "assert np.allclose(A, np.array([[-0.97586824, -0.96886028], [-0.72207398, -0.7756364 ]]), atol=1e-5)\n",
    "assert np.allclose(layers_cache[0][0], np.array([[ 0.88389311,  0.19586502],\n",
    "       [ 0.35753652, -2.34326191],\n",
    "       [-1.08483259,  0.55969629],\n",
    "       [ 0.93946935, -0.97848104]]), atol=1e-5)\n",
    "assert np.allclose(layers_cache[0][1], np.array([[ 0.50309684,  0.40641447,  0.32346101, -0.49341088],\n",
    "       [-0.79201679, -0.84236793, -1.27950266,  0.24571517],\n",
    "       [-0.0441948 ,  1.56763255,  1.05110868,  0.40636843]]), atol=1e-5)\n",
    "assert np.allclose(layers_cache[0][2], np.array([[-0.3930997 , -0.35860945],\n",
    "       [-2.57205022, -2.32750301],\n",
    "       [ 0.88304839, -2.37121977]]), atol=1e-5)\n",
    "assert np.allclose(layers_cache[1][0], np.array([[-0.03250385, -0.03013528],\n",
    "       [-0.09236212, -0.0902461 ],\n",
    "       [ 0.88304839, -0.09066332]]), atol=1e-5)\n",
    "assert np.allclose(layers_cache[1][1], np.array([[ 1.33277821, -0.24333877, -0.13003071],\n",
    "       [-0.10901737,  1.55618644,  0.12877835]]), atol=1e-5)\n",
    "assert np.allclose(layers_cache[1][2], np.array([[-2.20261728, -2.07336298],\n",
    "       [-0.91196483, -1.03432312]]), atol=1e-5)\n",
    "\n",
    "# print(A)\n",
    "# print(layers_cache)\n",
    "\n",
    "### END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eL9lQqMr1CSE",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-779d67c8e92d5063",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### 6- Cost Function (0.5 Marks):\n",
    "You can compute the cross-entropy cost using the following formula: $$-\\frac{1}{m} \\sum\\limits_{i = 1}^{m} (y^{(i)}*\\log\\left(a^{[H] (i)}\\right) + (1-y^{(i)})*\\log\\left(1- a^{[H](i)}\\right))$$\n",
    "where  \n",
    "$$m = number \\ of \\ examples$$\n",
    "$$ y^{(i)} = Ground \\ truth \\ labels \\ for \\ i-th \\ example $$\n",
    "$$ a^{[H](i)} = Output \\ layer \\ activations \\ for \\ i-th \\ example $$\n",
    "$$ * \\ denotes \\ elementwise \\ multiplication $$\n",
    "\n",
    "Below is a plot of the loss function, where $t$ denotes ground truth label and $y$ denotes the predicted label.\n",
    "\n",
    "<center><img src=\"cross_entropy_plot.png\" width=\"350\" height=\"350\"></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7JZRKq9dRIL0",
    "jupyter": {
     "outputs_hidden": true
    },
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-6d27debe15051a53",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "### 0.5 Marks\n",
    "def cross_entropy_cost(AH, Y):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "        AH - Activations of output layer, representing probability vector corresponding to your label predictions, of shape (1, number of examples)\n",
    "        Y - Ground truth \"label\" vector of shape (1, number of examples)\n",
    "    Outputs:\n",
    "        cost - Cross Entropy cost, of shape () i.e. scalar value\n",
    "    \"\"\"\n",
    "    # Compute cross entropy cost function in the variable 'cost'\n",
    "    ### BEGIN SOLUTION\n",
    "    m = Y.shape[1]\n",
    "    cost = (-1 / m) * np.sum(np.multiply(Y, np.log(AH)) + np.multiply(1 - Y, np.log(1 - AH)))\n",
    "    ### END SOLUTION\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "-uhjzdypeMy0",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-80f81ae4f3a1fb0f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "outputId": "191888d4-d453-4ecb-95c5-309accea92af"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running sample test case\n",
      "Sample test case passed\n"
     ]
    }
   ],
   "source": [
    "# do not change code here\n",
    "### SAMPLE TEST CASE\n",
    "print(\"Running sample test case\")\n",
    "AH = np.array([[0.6, 0.9, 0.4, 0.1]])\n",
    "Y = np.asarray([[1, 1, 1, 0]])\n",
    "cost = cross_entropy_cost(AH, Y)\n",
    "assert np.allclose(cost, 0.40945934673894957, atol=1e-5)\n",
    "print(\"Sample test case passed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LaUj80pleMsT",
    "jupyter": {
     "outputs_hidden": true
    },
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-6397d2ba54db427c",
     "locked": true,
     "points": 0.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# do not change code here\n",
    "# Hidden Test cases\n",
    "### BEGIN HIDDEN TESTS\n",
    "\n",
    "AH = np.array([[0.5, 0.5, 0.7, 0.3, 0.9]])\n",
    "Y = np.asarray([[1, 0, 1, 0, 1]])\n",
    "cost = cross_entropy_cost(AH, Y)\n",
    "assert cost.shape == ()\n",
    "assert np.allclose(cost, 0.4410009529310364, atol=1e-5)\n",
    "\n",
    "# print(cost)\n",
    "\n",
    "### END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0r2JlkCIVvs2",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-6fa18cdf415f912a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### 7- Backward Propagation (6 Marks):\n",
    "Backpropagation is used to calculate the gradient of the loss function with respect to the weights. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "psO6Bh3iolzL",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-fbb9e74a28d6fd31",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### 7.1- Activation Backward Propagation (3.5 Marks):\n",
    "This function computes  $dZ^{[h]} = \\frac{\\partial \\mathcal{L} }{\\partial Z^{[h]}}$, given  $dA^{[h]} = \\frac{\\partial \\mathcal{L} }{\\partial A^{[h]}}$ and $Z^{[h]}$as inputs. It is based on the following chain rule from calculus:\n",
    "$$ dZ^{[h]} = dA^{[h]} * \\frac{\\partial \\mathcal{A^{[h]}} }{\\partial Z^{[h]}}$$\n",
    "\n",
    "Note the following rules for different activation functions:\n",
    "$$ \\frac{\\partial \\mathcal{(relu(z))} }{\\partial z} =   \\left\\{\n",
    "\\begin{array}{ll}\n",
    "      1 & z>=0 \\\\\n",
    "      0& z < 0 \\\\\n",
    "\\end{array} \n",
    "\\right.  $$\n",
    "$$\\frac{\\partial \\mathcal{(sigmoid(z))} }{\\partial z} = sigmoid(z) * (1-sigmoid(z))$$\n",
    "$$\\frac{\\partial \\mathcal{(tanh(z))} }{\\partial z} = 1-tanh(z)*tanh(z)$$\n",
    "$$ \\frac{\\partial \\mathcal{(leaky\\_relu(z))} }{\\partial z} =   \\left\\{\n",
    "\\begin{array}{ll}\n",
    "      1 & z>=0 \\\\\n",
    "      \\alpha& z < 0 \\\\\n",
    "\\end{array} \n",
    "\\right.  $$\n",
    "$$\\frac{\\partial \\mathcal{(softplus(z))} }{\\partial z} = sigmoid(z) $$\n",
    "$$ \\frac{\\partial \\mathcal{(ELU(z))} }{\\partial z} =   \\left\\{\n",
    "\\begin{array}{ll}\n",
    "      1 & z>=0 \\\\\n",
    "      \\alpha*e^{z}& z < 0 \\\\\n",
    "\\end{array} \n",
    "\\right.  $$\n",
    "\n",
    "Hints:\n",
    "* You may find ```copy``` argument of np.array() to be useful(<a href=\"https://docs.scipy.org/doc/numpy/reference/generated/numpy.array.html\">numpy.array()</a>).\n",
    "* You may find NumPy Masking an useful concept while implementing some of the below functions (<a href=\"https://jakevdp.github.io/PythonDataScienceHandbook/02.06-boolean-arrays-and-masks.html#Boolean-Arrays-as-Masks\">NumPy Masking</a>)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PEuP5kox6-lf",
    "jupyter": {
     "outputs_hidden": true
    },
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-6d112a009f419516",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "### 0.5 Marks\n",
    "def linear_backward(dA, Z):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "        dA - Gradient of the cost function w.r.t. the post activation value A, of shape(size of current layer, number of examples)\n",
    "        Z - Pre activation value, of shape(size of current layer, number of examples)\n",
    "    Outputs:\n",
    "        dZ - Gradient of the cost function w.r.t. Z, of shape(size of current layer, number of examples)\n",
    "    \"\"\"\n",
    "    # Compute dZ\n",
    "    ### BEGIN SOLUTION \n",
    "    dZ = np.array(dA, copy=True)\n",
    "    ### END SOLUTION\n",
    "    return dZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-df72b2f6471f40c6",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running sample test case\n",
      "Sample test case passed\n"
     ]
    }
   ],
   "source": [
    "# do not change code here\n",
    "### SAMPLE TEST CASE\n",
    "print(\"Running sample test case\")\n",
    "np.random.seed(7)\n",
    "dA = np.random.randn(1,2)\n",
    "Z = np.random.randn(1,2)\n",
    "l_dZ = np.array([[ 1.6905257,  -0.46593737]])\n",
    "assert np.allclose(l_dZ, linear_backward(dA, Z), atol=1e-5)\n",
    "print(\"Sample test case passed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-0002eb6941a7e20c",
     "locked": true,
     "points": 0.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# do not change code here\n",
    "# Hidden Test cases\n",
    "### BEGIN HIDDEN TESTS\n",
    "\n",
    "np.random.seed(23)\n",
    "dA = np.random.randn(2,2)\n",
    "Z = np.random.randn(2,2)\n",
    "l_dZ = np.array([[ 0.66698806,  0.02581308], [-0.77761941, 0.94863382]])\n",
    "assert linear_backward(dA, Z).shape == (2, 2)\n",
    "assert np.allclose(l_dZ, linear_backward(dA, Z), atol=1e-5)\n",
    "\n",
    "# print(dA)\n",
    "# print(Z)\n",
    "# print(linear_backward(dA, Z))\n",
    "\n",
    "### END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WRo-iTVb6-hE",
    "jupyter": {
     "outputs_hidden": true
    },
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-e6f444238ea3c698",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "### 0.5 Marks\n",
    "def relu_backward(dA, Z):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "        dA - Gradient of the cost function w.r.t. the post activation value A, of shape(size of current layer, number of examples)\n",
    "        Z - Pre activation value, of shape(size of current layer, number of examples)\n",
    "    Outputs:\n",
    "        dZ - Gradient of the cost function w.r.t. Z, of shape(size of current layer, number of examples)\n",
    "    \"\"\"\n",
    "    # Compute dZ\n",
    "    ### BEGIN SOLUTION\n",
    "    dZ = np.array(dA, copy=True)\n",
    "    dZ[Z < 0] = 0\n",
    "    ### END SOLUTION\n",
    "    return dZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-6db88f5dc159c210",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running sample test case\n",
      "Sample test case passed\n"
     ]
    }
   ],
   "source": [
    "# do not change code here\n",
    "### SAMPLE TEST CASE\n",
    "print(\"Running sample test case\")\n",
    "np.random.seed(7)\n",
    "dA = np.random.randn(1,2)\n",
    "Z = np.random.randn(1,2)\n",
    "r_dZ = np.array([[ 1.6905257,  -0.46593737]])\n",
    "assert np.allclose(r_dZ, relu_backward(dA, Z), atol=1e-5)\n",
    "print(\"Sample test case passed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-0f18ed800ca0ffb0",
     "locked": true,
     "points": 0.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# do not change code here\n",
    "# Hidden Test cases\n",
    "### BEGIN HIDDEN TESTS\n",
    "\n",
    "np.random.seed(23)\n",
    "dA = np.random.randn(2,2)\n",
    "Z = np.random.randn(2,2)\n",
    "r_dZ = np.array([[0.66698806, 0.        ], [0.,         0.        ]])\n",
    "assert relu_backward(dA, Z).shape == (2, 2)\n",
    "assert np.allclose(r_dZ, relu_backward(dA, Z), atol=1e-5)\n",
    "\n",
    "# print(dA)\n",
    "# print(Z)\n",
    "# print(relu_backward(dA, Z))\n",
    "\n",
    "### END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Q10_Gd4t6-aj",
    "jupyter": {
     "outputs_hidden": true
    },
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-13ad0c5972c542d5",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "### 0.5 Marks\n",
    "def sigmoid_backward(dA, Z):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "        dA - Gradient of the cost function w.r.t. the post activation value A, of shape(size of current layer, number of examples)\n",
    "        Z - Pre activation value, of shape(size of current layer, number of examples)\n",
    "    Outputs:\n",
    "        dZ - Gradient of the cost function w.r.t. Z, of shape(size of current layer, number of examples)\n",
    "    \"\"\"\n",
    "    # Compute dZ\n",
    "    ### BEGIN SOLUTION\n",
    "    A = 1/(1+np.exp(-Z))\n",
    "    dZ = dA * A * (1-A)\n",
    "    ### END SOLUTION\n",
    "    return dZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-9f2a28734ccbbfc7",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running sample test case\n",
      "Sample test case passed\n"
     ]
    }
   ],
   "source": [
    "# do not change code here\n",
    "### SAMPLE TEST CASE\n",
    "print(\"Running sample test case\")\n",
    "np.random.seed(7)\n",
    "dA = np.random.randn(1,2)\n",
    "Z = np.random.randn(1,2)\n",
    "s_dZ = np.array([[ 0.42251764, -0.11177899]])\n",
    "assert np.allclose(s_dZ, sigmoid_backward(dA, Z), atol=1e-5)\n",
    "print(\"Sample test case passed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-beddd2c15d5314f5",
     "locked": true,
     "points": 0.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# do not change code here\n",
    "# Hidden Test cases\n",
    "### BEGIN HIDDEN TESTS\n",
    "\n",
    "np.random.seed(23)\n",
    "dA = np.random.randn(2,2)\n",
    "Z = np.random.randn(2,2)\n",
    "s_dZ = np.array([[ 0.14779661,  0.00495424], [-0.18798428,  0.17439827]])\n",
    "assert sigmoid_backward(dA, Z).shape == (2, 2)\n",
    "assert np.allclose(s_dZ, sigmoid_backward(dA, Z), atol=1e-5)\n",
    "\n",
    "# print(dA)\n",
    "# print(Z)\n",
    "# print(sigmoid_backward(dA, Z))\n",
    "\n",
    "### END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sFnXRdKQ6-UG",
    "jupyter": {
     "outputs_hidden": true
    },
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-a514cc92b868d5e0",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "### 0.5 Marks\n",
    "def tanh_backward(dA, Z):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "        dA - Gradient of the cost function w.r.t. the post activation value A, of shape(size of current layer, number of examples)\n",
    "        Z - Pre activation value, of shape(size of current layer, number of examples)\n",
    "    Outputs:\n",
    "        dZ - Gradient of the cost function w.r.t. Z, of shape(size of current layer, number of examples)\n",
    "    \"\"\"\n",
    "    # Compute dZ\n",
    "    ### BEGIN SOLUTION\n",
    "    A = tanh(Z)\n",
    "    dZ = dA * (1 - A**2) \n",
    "    ### END SOLUTION\n",
    "    return dZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-7afc384db94453c1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running sample test case\n",
      "Sample test case passed\n"
     ]
    }
   ],
   "source": [
    "# do not change code here\n",
    "### SAMPLE TEST CASE\n",
    "print(\"Running sample test case\")\n",
    "np.random.seed(7)\n",
    "dA = np.random.randn(1,2)\n",
    "Z = np.random.randn(1,2)\n",
    "t_dZ = np.array([[ 1.68870604, -0.39638438]])\n",
    "assert np.allclose(t_dZ, tanh_backward(dA, Z), atol=1e-5)\n",
    "print(\"Sample test case passed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-6a3bd98151b05f60",
     "locked": true,
     "points": 0.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# do not change code here\n",
    "# Hidden Test cases\n",
    "### BEGIN HIDDEN TESTS\n",
    "\n",
    "np.random.seed(23)\n",
    "dA = np.random.randn(2,2)\n",
    "Z = np.random.randn(2,2)\n",
    "t_dZ = np.array([[ 0.42250832,  0.01001862], [-0.68135397,  0.32075693]])\n",
    "assert tanh_backward(dA, Z).shape == (2, 2)\n",
    "assert np.allclose(t_dZ, tanh_backward(dA, Z), atol=1e-5)\n",
    "\n",
    "# print(dA)\n",
    "# print(Z)\n",
    "# print(tanh_backward(dA, Z))\n",
    "\n",
    "### END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qq_8Y5vD6-PL",
    "jupyter": {
     "outputs_hidden": true
    },
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-1a0b272ca6a5609e",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "### 0.5 Marks\n",
    "def leaky_relu_backward(dA, Z, alpha=0.1):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "        dA - Gradient of the cost function w.r.t. the post activation value A, of shape(size of current layer, number of examples)\n",
    "        Z - Pre activation value, of shape(size of current layer, number of examples)\n",
    "        alpha - A hyperparameter\n",
    "    Outputs:\n",
    "        dZ - Gradient of the cost function w.r.t. Z, of shape(size of current layer, number of examples)\n",
    "    \"\"\"\n",
    "    # Compute dZ\n",
    "    ### BEGIN SOLUTION\n",
    "    dZ = np.array(dA, copy=True)\n",
    "    neg_indices = Z < 0\n",
    "    dZ[neg_indices] = alpha * dA[neg_indices]\n",
    "    ### END SOLUTION\n",
    "    return dZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-5e7672e6d0d90917",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running sample test case\n",
      "Sample test case passed\n"
     ]
    }
   ],
   "source": [
    "# do not change code here\n",
    "### SAMPLE TEST CASE\n",
    "print(\"Running sample test case\")\n",
    "np.random.seed(7)\n",
    "dA = np.random.randn(1,2)\n",
    "Z = np.random.randn(1,2)\n",
    "lr_dZ = np.array([[ 1.6905257,  -0.46593737]])\n",
    "assert np.allclose(lr_dZ, leaky_relu_backward(dA, Z), atol=1e-5)\n",
    "print(\"Sample test case passed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-38da401f45edbbf7",
     "locked": true,
     "points": 0.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# do not change code here\n",
    "# Hidden Test cases\n",
    "### BEGIN HIDDEN TESTS\n",
    "\n",
    "np.random.seed(23)\n",
    "dA = np.random.randn(2,2)\n",
    "Z = np.random.randn(2,2)\n",
    "lr_dZ = np.array([[ 0.66698806,  0.00258131], [-0.07776194,  0.09486338]])\n",
    "assert leaky_relu_backward(dA, Z).shape == (2, 2)\n",
    "assert np.allclose(lr_dZ, leaky_relu_backward(dA, Z), atol=1e-5)\n",
    "\n",
    "# print(dA)\n",
    "# print(Z)\n",
    "# print(leaky_relu_backward(dA, Z))\n",
    "\n",
    "### END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sRc6oDPx6-Ib",
    "jupyter": {
     "outputs_hidden": true
    },
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-485d07f0ad8549d5",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "### 0.5 Marks\n",
    "def softplus_backward(dA, Z):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "        dA - Gradient of the cost function w.r.t. the post activation value A, of shape(size of current layer, number of examples)\n",
    "        Z - Pre activation value, of shape(size of current layer, number of examples)\n",
    "    Outputs:\n",
    "        dZ - Gradient of the cost function w.r.t. Z, of shape(size of current layer, number of examples)\n",
    "    \"\"\"\n",
    "    # Compute dZ\n",
    "    ### BEGIN SOLUTION \n",
    "    dZ = dA * sigmoid(Z) \n",
    "    ### END SOLUTION\n",
    "    return dZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-ced258b7cc72a9c6",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running sample test case\n",
      "Sample test case passed\n"
     ]
    }
   ],
   "source": [
    "# do not change code here\n",
    "### SAMPLE TEST CASE\n",
    "print(\"Running sample test case\")\n",
    "np.random.seed(7)\n",
    "dA = np.random.randn(1,2)\n",
    "Z = np.random.randn(1,2)\n",
    "st_dZ = np.array([[ 0.85913244, -0.27979175]])\n",
    "assert np.allclose(st_dZ, softplus_backward(dA, Z), atol=1e-5)\n",
    "print(\"Sample test case passed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-6edeaf77c3029679",
     "locked": true,
     "points": 0.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# do not change code here\n",
    "# Hidden Test cases\n",
    "### BEGIN HIDDEN TESTS\n",
    "\n",
    "np.random.seed(23)\n",
    "dA = np.random.randn(2,2)\n",
    "Z = np.random.randn(2,2)\n",
    "st_dZ = np.array([[ 0.44592042,  0.00668604], [-0.31815024,  0.23031603]])\n",
    "assert softplus_backward(dA, Z).shape == (2, 2)\n",
    "assert np.allclose(st_dZ, softplus_backward(dA, Z), atol=1e-5)\n",
    "\n",
    "# print(dA)\n",
    "# print(Z)\n",
    "# print(softplus_backward(dA, Z))\n",
    "\n",
    "### END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4O4FK6OK6-A0",
    "jupyter": {
     "outputs_hidden": true
    },
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-d8609867fb7002d7",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "### 0.5 Marks\n",
    "def ELU_backward(dA, Z, alpha=0.1):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "        dA - Gradient of the cost function w.r.t. the post activation value A, of shape(size of current layer, number of examples)\n",
    "        Z - Pre activation value, of shape(size of current layer, number of examples)\n",
    "        alpha - A hyperparameter\n",
    "    Outputs:\n",
    "        dZ - Gradient of the cost function w.r.t. Z, of shape(size of current layer, number of examples)\n",
    "    \"\"\"\n",
    "    # Compute dZ\n",
    "    ### BEGIN SOLUTION \n",
    "    dZ = np.array(dA, copy=True)\n",
    "    neg_indices = Z < 0\n",
    "    dZ[neg_indices] = alpha * dA[neg_indices] * np.exp(Z[neg_indices])\n",
    "    ### END SOLUTION\n",
    "    return dZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-85eb15f13fd1e6e5",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running sample test case\n",
      "Sample test case passed\n"
     ]
    }
   ],
   "source": [
    "# do not change code here\n",
    "### SAMPLE TEST CASE\n",
    "print(\"Running sample test case\")\n",
    "np.random.seed(7)\n",
    "dA = np.random.randn(1,2)\n",
    "Z = np.random.randn(1,2)\n",
    "e_dZ = np.array([[ 1.6905257,  -0.46593737]])\n",
    "assert np.allclose(e_dZ, ELU_backward(dA, Z), atol=1e-5)\n",
    "print(\"Sample test case passed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-48b1999d45d4b382",
     "locked": true,
     "points": 0.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# do not change code here\n",
    "# Hidden Test cases\n",
    "### BEGIN HIDDEN TESTS\n",
    "\n",
    "np.random.seed(23)\n",
    "dA = np.random.randn(2,2)\n",
    "Z = np.random.randn(2,2)\n",
    "e_dZ = np.array([[ 0.66698806,  0.00090232], [-0.0538447,   0.03041628]])\n",
    "assert ELU_backward(dA, Z).shape == (2, 2)\n",
    "assert np.allclose(e_dZ, ELU_backward(dA, Z), atol=1e-5)\n",
    "\n",
    "# print(dA)\n",
    "# print(Z)\n",
    "# print(ELU_backward(dA, Z))\n",
    "\n",
    "### END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jKxjvqsIVzcz",
    "jupyter": {
     "outputs_hidden": true
    },
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-3dea7b75aa891a86",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# do not change code here\n",
    "def activation_backward(dA, Z, activation, alpha=0.1):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "        dA - Gradient of the cost function w.r.t. the post activation value A, of shape(size of current layer, number of examples)\n",
    "        Z - Pre activation value, of shape(size of current layer, number of examples)\n",
    "        activation - Python string denoting the activation function used in the current layer \n",
    "        alpha - A hyperparameter for activation functions leaky_relu and ELU.\n",
    "    Outputs:\n",
    "        dZ - Gradient of the cost function w.r.t. Z, of shape(size of current layer, number of examples)\n",
    "    \"\"\"\n",
    "    if activation == \"relu\":\n",
    "        dZ = relu_backward(dA, Z)\n",
    "    elif activation == \"sigmoid\":\n",
    "        dZ = sigmoid_backward(dA, Z)\n",
    "    elif activation == \"tanh\":\n",
    "        dZ = tanh_backward(dA, Z)\n",
    "    elif activation == \"leaky_relu\":\n",
    "        dZ = leaky_relu_backward(dA, Z, alpha)\n",
    "    elif activation == \"softplus\":\n",
    "        dZ = softplus_backward(dA, Z)\n",
    "    elif activation == \"ELU\":\n",
    "        dZ = ELU_backward(dA, Z, alpha)\n",
    "    else: # Linear activation\n",
    "        dZ = linear_backward(dA, Z)   \n",
    "    return dZ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cuo7oGE9AhB-",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-6c24044490127782",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### 7.2- Backward Propagation Step (1 Mark):\n",
    "* The following function computes $dA^{[h-1]}, dW^{[h]}$ and $db^{[h]}$, given $dA^{[h]}, Z^{[h]}, W^{[h]}$ and $A^{[h-1]}$ as inputs.  \n",
    "* We can calculate $dZ^{[h]}$ using $dA^{[h]}$ and $Z^{[h]}$ as inputs to the ```activation_backward()``` function defined above.  \n",
    "* The three outputs $(dA^{[h-1]}, dW^{[h]}, db^{[h]})$ are computed using $dZ^{[h]}, W^{[h]}$ and $A^{[h-1]}$ using the below equations(using the fact that $ Z^{[h]} = W^{[h]}.A^{[h-1]}+b^{[h]}$):  \n",
    "  - $ dA^{[h-1]} = \\frac{\\partial \\mathcal{L} }{\\partial A^{[h-1]}} = W^{[h] T} dZ^{[h]}$\n",
    "  - $ dW^{[h]} = \\frac{\\partial \\mathcal{L} }{\\partial W^{[h]}} = \\frac{1}{m} dZ^{[h]} A^{[h-1] T}$\n",
    "  - $ db^{[h]} = \\frac{\\partial \\mathcal{L} }{\\partial b^{[h]}} = \\frac{1}{m} \\sum_{i = 1}^{m} dZ^{[h](i)}$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EWdfqv1jZEkj",
    "jupyter": {
     "outputs_hidden": true
    },
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-c0afcf272ad25d13",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "### 1 Mark\n",
    "def backward_step(dA, layer_cache, activation, alpha=0.1):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "        dA - Gradient of the cost function w.r.t. the post activation value A for current layer\n",
    "        layer_cache - A list [A_prev, W, Z] for current layer, where A_prev is activation of prev layer, W is weight matrix of current layer, and Z is pre-activation value for current layer\n",
    "        activation - Python string denoting the activation function used in the current layer\n",
    "        alpha - A hyperparameter for activation functions leaky_relu and ELU.\n",
    "    Outputs:\n",
    "        dA_prev - Gradient of the cost function w.r.t. the activation (of the previous layer h-1), of same shape as A_prev\n",
    "        dW - Gradient of the cost function w.r.t. W (current layer l), of same shape as W\n",
    "        db - Gradient of the cost function w.r.t. b (current layer l), of same shape as b\n",
    "    \"\"\"\n",
    "    A_prev = layer_cache[0]\n",
    "    W = layer_cache[1]\n",
    "    Z = layer_cache[2]\n",
    "    # 1. Compute dZ by calling activation_backward(). \n",
    "    # 2. Compute dA_prev, dW and db using the equations specified above\n",
    "    ### BEGIN SOLUTION\n",
    "    m = A_prev.shape[1] # Number of examples\n",
    "    dZ = activation_backward(dA, Z, activation, alpha)\n",
    "    dW = np.dot(dZ, A_prev.T) / m\n",
    "    db = (np.sum(dZ, axis=1, keepdims=True)) / m\n",
    "    dA_prev = np.dot(W.T, dZ)\n",
    "    ### END SOLUTION\n",
    "    \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "tnAPn6zhVN25",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-97c7994ca5564038",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "outputId": "3f28ff6c-e35e-4f79-b9c5-4faa5cc9a4e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running sample test case\n",
      "Sample test case passed\n"
     ]
    }
   ],
   "source": [
    "# do not change code here\n",
    "### SAMPLE TEST CASE\n",
    "print(\"Running sample test case\")\n",
    "np.random.seed(9)\n",
    "dA = np.random.randn(1,2)\n",
    "A_prev = np.random.randn(3,2)\n",
    "W = np.random.randn(1,3)\n",
    "Z = np.random.randn(1,2)\n",
    "layer_cache = [A_prev, W, Z]\n",
    "dA_prev, dW, db = backward_step(dA, layer_cache, \"linear\")\n",
    "assert np.allclose(dA_prev, np.array([[-0.00026681, 0.06968763], [-0.00071829, 0.18760934], [0.00070492, -0.18411849]]), atol=1e-5)\n",
    "assert np.allclose(dW, np.array([[0.00124645, 0.06944523, 0.07022351]]), atol=1e-5)\n",
    "assert np.allclose(db, np.array([[-0.14421776]]), atol=1e-5)\n",
    "print(\"Sample test case passed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ASeDmtb9VNs3",
    "jupyter": {
     "outputs_hidden": true
    },
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-a546893a7dcefb85",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# do not change code here\n",
    "# Hidden Test cases\n",
    "### BEGIN HIDDEN TESTS\n",
    "\n",
    "np.random.seed(11)\n",
    "dA = np.random.randn(2,2)\n",
    "A_prev = np.random.randn(3,2)\n",
    "W = np.random.randn(2,3)\n",
    "Z = np.random.randn(2,2)\n",
    "layer_cache = [A_prev, W, Z]\n",
    "dA_prev, dW, db = backward_step(dA, layer_cache, \"linear\")\n",
    "assert dA_prev.shape == (3, 2)\n",
    "assert dW.shape == (2, 3)\n",
    "assert db.shape == (2, 1)\n",
    "assert np.allclose(dA_prev, np.array([[-1.82237035, -1.23549238], [-0.19965093,  3.6001306 ], [ 1.74902667,  2.773025  ]]), atol=1e-5)\n",
    "assert np.allclose(dW, np.array([[ 0.03847216, -0.51451848,  0.5207247 ], [ 0.42604914, -0.28841594,  1.31167883]]), atol=1e-5)\n",
    "assert np.allclose(db, np.array([[ 0.73169087], [-1.56894185]]), atol=1e-5)\n",
    "\n",
    "# print(dA_prev)\n",
    "# print(dW)\n",
    "# print(db)\n",
    "\n",
    "### END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZAYPzzOQO3Ix",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-edd5f2bcaea18096",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### 7.3- Backward Propagation Module (1.5 Marks):\n",
    "* This function computes the gradients of loss function w.r.t. all the weights of the neural network.\n",
    "* To backpropagate through this network, we know that the output is, $A^{[H]} = sigmoid(Z^{[H]})$. Your code thus needs to compute $ dA^{[H]} = \\frac{\\partial \\mathcal{L}}{\\partial A^{[H]}}$. To do so, use this formula:\n",
    "$$ dA^{[H]} = -(Y / A^{[H]} - (1-Y)/(1-A^{[H]}))$$\n",
    "where $ \"/\" \\ denotes \\ elementwise \\ division$\n",
    "* You can then use this post-activation gradient $dA^{[H]}$ to keep going backward by repeatedly calling ```backward_step()``` in a for-loop repeated $H$ times.\n",
    "* The output of this function is a list ```gradients``` of length $H$, whose each element is a list [$dW^{[h]}, db^{[h]}$].\n",
    "* For the sake of clarity, consider a single-hidden layer NN with input of size 5, hidden layer size = 4 and output layer size =3. For this example, the output ```gradients``` will be [[$dW^{[1]}, db^{[1]}$], [$dW^{[2]}, db^{[2]}$]]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6OGsL2eOCXwG",
    "jupyter": {
     "outputs_hidden": true
    },
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-7f3167fd47b50fc2",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "### 1.5 Marks\n",
    "def backward_module(AH, Y, layers_cache, activations, alpha=0.1):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "        AH - Activations of output layer, representing probability vector corresponding to your label predictions, of shape (1, number of examples)\n",
    "        Y - Ground truth \"label\" vector of shape (1, number of examples)\n",
    "        layers_cache - A list (of length H), where each element is a list [A_prev, W, Z], representing the values A_prev (input to the layer), W (Weight matrix of the layer)\n",
    "            and Z (pre-activation value) for layer h. The purpose of cache is to store values that will be needed during backpropagation.\n",
    "        activations - A list (of length H), where each element is a Python string representing the activation function to be used in the corresponding layer\n",
    "        alpha - A hyperparameter for activation functions leaky_relu and ELU.\n",
    "    Outputs:\n",
    "        gradients - A list (of length H), where each element is a list [dW, db], representing the values dW (gradient of cost function w.r.t. W) and  \n",
    "            db (gradient of cost function w.r.t. b)\n",
    "    \"\"\"\n",
    "    gradients = []\n",
    "    H = len(layers_cache) # the number of layers\n",
    "    m = AH.shape[1]\n",
    "    Y = Y.reshape(AH.shape) \n",
    "    \n",
    "    # 1. Initializing the backpropagation - Compute dAH.\n",
    "    # 2. Use a for loop repeated H times - In each iteration, compute dW and db by using backward_step()[supplied with appropriate input parameters by using \n",
    "    # the two lists - layers_cache and activations], and insert them in the gradients list in the manner specified above.\n",
    "    ### BEGIN SOLUTION\n",
    "    dAH = - (np.divide(Y, AH) - np.divide(1 - Y, 1 - AH))\n",
    "    dA_prev_temp = dAH\n",
    "    for h in reversed(range(H)):\n",
    "        current_layer_cache = layers_cache[h]\n",
    "        dA_prev_temp, dW_temp, db_temp = backward_step(dA_prev_temp, current_layer_cache, activations[h], alpha)\n",
    "        gradients.insert(0, list([dW_temp, db_temp]))\n",
    "    ### END SOLUTION\n",
    "    \n",
    "    return gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "KD-Ar6QqWJ0u",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-dc66a6e4bd6b154e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "outputId": "f1974086-cd66-4900-f873-48e9db9708c8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running sample test case\n",
      "Sample test case passed\n"
     ]
    }
   ],
   "source": [
    "# do not change code here\n",
    "### SAMPLE TEST CASE\n",
    "print(\"Running sample test case\")\n",
    "np.random.seed(5)\n",
    "AH = np.random.randn(1, 3)\n",
    "Y = np.array([[1, 0, 1]])\n",
    "A0 = np.random.randn(4,3)\n",
    "W1 = np.random.randn(2,4)\n",
    "Z1 = np.random.randn(2,3)\n",
    "A1 = np.random.randn(2,3)\n",
    "W2 = np.random.randn(1,2)\n",
    "Z2 = np.random.randn(1,3)\n",
    "layers_cache = [[A0, W1, Z1], [A1, W2, Z2]]\n",
    "activations = [\"relu\", \"sigmoid\"]\n",
    "gradients = backward_module(AH, Y, layers_cache, activations)\n",
    "assert np.allclose(gradients[0][0], np.array([[-6.04541079e-03, -1.01341276e-02, -1.75742363e-02, 1.75552561e-02],\n",
    "       [ 6.03863058e-05,  2.16659967e-02,  8.92094963e-03, 1.53067729e-02]]), atol=1e-5)\n",
    "assert np.allclose(gradients[0][1], np.array([[ 0.01067648], [-0.02851614]]), atol=1e-5)\n",
    "assert np.allclose(gradients[1][0], np.array([[ 0.07517931, -0.00451628]]), atol=1e-5)\n",
    "assert np.allclose(gradients[1][1], np.array([[-0.08177088]]), atol=1e-5)\n",
    "print(\"Sample test case passed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "C9YIUNKLWJlC",
    "jupyter": {
     "outputs_hidden": true
    },
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-0d2b5f1be41f422e",
     "locked": true,
     "points": 1.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# do not change code here\n",
    "# Hidden Test cases\n",
    "### BEGIN HIDDEN TESTS\n",
    "\n",
    "np.random.seed(5)\n",
    "AH = np.random.randn(1, 2)\n",
    "Y = np.array([[1, 0]])\n",
    "A0 = np.random.randn(4,2)\n",
    "W1 = np.random.randn(3,4)\n",
    "Z1 = np.random.randn(3,2)\n",
    "A1 = np.random.randn(3,2)\n",
    "W2 = np.random.randn(1,3)\n",
    "Z2 = np.random.randn(1,2)\n",
    "layers_cache = [[A0, W1, Z1], [A1, W2, Z2]]\n",
    "activations = [\"relu\", \"sigmoid\"]\n",
    "gradients = backward_module(AH, Y, layers_cache, activations)\n",
    "assert gradients[0][0].shape == (3, 4)\n",
    "assert gradients[0][1].shape == (3, 1)\n",
    "assert gradients[1][0].shape == (1, 3)\n",
    "assert gradients[1][1].shape == (1, 1)\n",
    "assert np.allclose(gradients[0][0], np.array([[ 0.        ,  0.        ,  0.        ,  0.        ],\n",
    "       [-0.11912194,  0.03186602,  0.02853205, -0.01644594],\n",
    "       [-0.08980702, -0.00404963,  0.0335924 , -0.00693117]]), atol=1e-5)\n",
    "assert np.allclose(gradients[0][1], np.array([[ 0.        ],\n",
    "       [-0.02321956],\n",
    "       [-0.03694589]]), atol=1e-5)\n",
    "assert np.allclose(gradients[1][0], np.array([[-0.01054535, -0.20744224, -0.00831681]]), atol=1e-5)\n",
    "assert np.allclose(gradients[1][1], np.array([[-0.09316979]]), atol=1e-5)\n",
    "\n",
    "# print(gradients)\n",
    "\n",
    "### END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MoO0SjNqWQ27",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-9e18d437b25161c9",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### 8- Batch Gradient Descent Optimizer (0.5 Marks):\n",
    "Now that we have calculated the gradients of the loss function w.r.t. all the weights of the network, update the weights of the model, using gradient descent: \n",
    "\n",
    "$$ W^{[h]} = W^{[h]} - \\alpha \\text{ } dW^{[h]} $$\n",
    "$$ b^{[h]} = b^{[h]} - \\alpha \\text{ } db^{[h]} $$\n",
    "\n",
    "where $\\alpha$ is the learning rate.   \n",
    "After computing the updated weights, store them back in the weights list. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JQ7SSYlEWR1D",
    "jupyter": {
     "outputs_hidden": true
    },
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-f8f764ba09dbc3dd",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "### 0.5 Marks\n",
    "def update_weights(weights, gradients, lr):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "        weights - A list (of length H), where each element is a list - [Wh, bh], representing the weights matrix and bias vector of layer h (Both Wh and bh are numpy arrays).\n",
    "        gradients - A list (of length H), where each element is a list - [dWh, dbh], representing the gradients of cost function w.r.t. weights matrix and bias vector \n",
    "            of layer h respectively (Both dWh and dbh are numpy arrays).\n",
    "        lr - Learning Rate.\n",
    "    Outputs:\n",
    "        weights - A list (of length H), containing the updated weights.\n",
    "    \"\"\"\n",
    "    H = len(weights)\n",
    "    for h in range(H):\n",
    "        ### BEGIN SOLUTION\n",
    "        weights[h][0] = weights[h][0] - lr * gradients[h][0]\n",
    "        weights[h][1] = weights[h][1] - lr * gradients[h][1]\n",
    "        ### END SOLUTION      \n",
    "    \n",
    "    return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "Q3ei0vwKug4Q",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-a153a2912eed0ef9",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "outputId": "9bd0ee53-f4e7-4171-a5e7-ab5372e55824"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running sample test case\n",
      "Sample test case passed\n"
     ]
    }
   ],
   "source": [
    "# do not change code here\n",
    "### SAMPLE TEST CASE\n",
    "print(\"Running sample test case\")\n",
    "np.random.seed(4)\n",
    "W1 = np.random.randn(2,4)\n",
    "b1 = np.random.randn(2,1)\n",
    "W2 = np.random.randn(1,2)\n",
    "b2 = np.random.randn(1,1)\n",
    "weights = [[W1, b1], [W2, b2]]\n",
    "np.random.seed(1)\n",
    "dW1 = np.random.randn(2,4)\n",
    "db1 = np.random.randn(2,1)\n",
    "dW2 = np.random.randn(1,2)\n",
    "db2 = np.random.randn(1,1)\n",
    "gradients = [[dW1, db1], [dW2, db2]]\n",
    "new_weights = update_weights(weights, gradients, 0.2)\n",
    "assert np.allclose(new_weights[0][0], np.array([[-0.27430737,  0.62230262, -0.89027458,  0.90819223],\n",
    "       [-0.59138305, -1.1242695 , -0.99666912,  0.75081655]]), atol=1e-5)\n",
    "assert np.allclose(new_weights[0][1], np.array([[ 0.26844221],\n",
    "       [-1.09760256]]), atol=1e-5)\n",
    "assert np.allclose(new_weights[1][0], np.array([[0.3262481 , 0.32404121]]), atol=1e-5)\n",
    "assert np.allclose(new_weights[1][1], np.array([[0.48955584]]), atol=1e-5)\n",
    "print(\"Sample test case passed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ClAcor73ugt2",
    "jupyter": {
     "outputs_hidden": true
    },
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-1700e1aacd6db06f",
     "locked": true,
     "points": 0.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# do not change code here\n",
    "# Hidden Test cases\n",
    "### BEGIN HIDDEN TESTS\n",
    "\n",
    "np.random.seed(1)\n",
    "W1 = np.random.randn(3,4)\n",
    "b1 = np.random.randn(3,1)\n",
    "W2 = np.random.randn(1,3)\n",
    "b2 = np.random.randn(1,1)\n",
    "weights = [[W1, b1], [W2, b2]]\n",
    "np.random.seed(4)\n",
    "dW1 = np.random.randn(3,4)\n",
    "db1 = np.random.randn(3,1)\n",
    "dW2 = np.random.randn(1,3)\n",
    "db2 = np.random.randn(1,1)\n",
    "gradients = [[dW1, db1], [dW2, db2]]\n",
    "new_weights = update_weights(weights, gradients, 0.2)\n",
    "assert new_weights[0][0].shape == (3, 4)\n",
    "assert new_weights[0][1].shape == (3, 1)\n",
    "assert new_weights[1][0].shape == (1, 3)\n",
    "assert new_weights[1][1].shape == (1, 1)\n",
    "assert np.allclose(new_weights[0][0], np.array([[ 1.61423302, -0.71174668, -0.32898997, -1.21168832],\n",
    "       [ 0.94906793, -1.98462325,  1.87435312, -0.88092194],\n",
    "       [ 0.25258909, -0.01987505,  1.338374  , -2.04254332]]), atol=1e-5)\n",
    "assert np.allclose(new_weights[0][1], np.array([[-0.40743168],\n",
    "       [-0.45050498],\n",
    "       [ 1.36513269]]), atol=1e-5)\n",
    "assert np.allclose(new_weights[1][0], np.array([[-1.1700907 , -0.05105075, -1.18725428]]), atol=1e-5)\n",
    "assert np.allclose(new_weights[1][1], np.array([[-0.10245458]]), atol=1e-5)\n",
    "\n",
    "# print(new_weights)\n",
    "\n",
    "### END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sf_x8sL5ZjqC",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-d2cd7e1fa9faaf92",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### 9- Train a H-layer Neural Network:\n",
    "This function performs the following steps:  \n",
    "1. Initialize the weights of the neural network .\n",
    "2. Perform forward propagation on input argument ```X```.\n",
    "3. Compute cross entropy loss function.\n",
    "4. Perform backward propagation to compute gradients of loss function w.r.t. all the weights of the network.\n",
    "5. Use the gradients computed in step 4 to update the weights of the network.\n",
    "6. Append cross entropy cost's value to the list ```epoch_wise_costs```.\n",
    "7. Print (epoch, cross entropy cost's value) for epoch = 0, 100, 200, ...\n",
    "8. Repeat steps 2-7 for ```epochs```(input argument to below function) number of iterations.\n",
    "9. Return the weights of the neural network.\n",
    "\n",
    "We have provided the full function. All you have to do is to run the below cells to construct your neural network and train it on training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GTwxjlZHZ62P",
    "jupyter": {
     "outputs_hidden": true
    },
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-7e281ac96d1976be",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# do not change code here\n",
    "def H_layer_NN(X, Y, layers_sizes, activations, epochs=200, lr=0.001, alpha=0.1):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "        X - Input data, numpy array of shape (number_of_features_in_input_data, number of examples)\n",
    "        Y - Ground truth \"label\" vector of shape (1, number of examples)\n",
    "        layers_sizes - A list (of length H+1) containing the size (number of neurons) of each layer. Element at index 0 represents input layer size.\n",
    "        activations - A list (of length H), where each element is a Python string representing the activation function to be used in the corresponding layer\n",
    "        epochs - number of epochs of the training loop\n",
    "        lr - learning rate of the gradient descent update rule\n",
    "        alpha - A hyperparameter for activation functions leaky_relu and ELU.\n",
    "    Outputs:\n",
    "        weights - weights learnt by the model (can be used for prediction). A list (of length H), where each element is a list - [Wh, bh], representing the weights matrix and bias vector of layer h.\n",
    "    \"\"\"\n",
    "    epoch_wise_costs = []\n",
    "    \n",
    "    weights = construct_NN(layers_sizes)\n",
    "    for epoch in range(epochs):\n",
    "        AH, layers_cache = forward_module(X, weights, activations, alpha)\n",
    "        cost = cross_entropy_cost(AH, Y)\n",
    "        gradients = backward_module(AH, Y, layers_cache, activations, alpha)\n",
    "        update_weights(weights, gradients, lr)\n",
    "        epoch_wise_costs.append(cost)\n",
    "        if epoch % 100 == 0:\n",
    "            print(\"Cost after epoch %i: %f\" %(epoch, cost))\n",
    "    \n",
    "    plt.plot(np.array(epoch_wise_costs))\n",
    "    plt.ylabel('Cost Function')\n",
    "    plt.xlabel('Number of Epochs')\n",
    "    plt.title(\"Training Curve\")\n",
    "    \n",
    "    return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vIMztojuyZxi",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after epoch 0: 0.693178\n",
      "Cost after epoch 100: 0.692526\n",
      "Cost after epoch 200: 0.692375\n",
      "Cost after epoch 300: 0.692322\n",
      "Cost after epoch 400: 0.692267\n",
      "Cost after epoch 500: 0.692157\n",
      "Cost after epoch 600: 0.691907\n",
      "Cost after epoch 700: 0.691331\n",
      "Cost after epoch 800: 0.689997\n",
      "Cost after epoch 900: 0.686945\n",
      "Cost after epoch 1000: 0.680106\n",
      "Cost after epoch 1100: 0.665625\n",
      "Cost after epoch 1200: 0.638563\n",
      "Cost after epoch 1300: 0.597831\n",
      "Cost after epoch 1400: 0.552196\n",
      "Cost after epoch 1500: 0.514883\n",
      "Cost after epoch 1600: 0.490951\n",
      "Cost after epoch 1700: 0.476997\n",
      "Cost after epoch 1800: 0.468710\n",
      "Cost after epoch 1900: 0.463591\n",
      "Cost after epoch 2000: 0.460281\n",
      "Cost after epoch 2100: 0.457921\n",
      "Cost after epoch 2200: 0.456061\n",
      "Cost after epoch 2300: 0.454427\n",
      "Cost after epoch 2400: 0.453068\n",
      "Cost after epoch 2500: 0.451888\n",
      "Cost after epoch 2600: 0.450734\n",
      "Cost after epoch 2700: 0.449555\n",
      "Cost after epoch 2800: 0.448516\n",
      "Cost after epoch 2900: 0.447597\n",
      "Cost after epoch 3000: 0.446764\n",
      "Cost after epoch 3100: 0.445996\n",
      "Cost after epoch 3200: 0.445285\n",
      "Cost after epoch 3300: 0.444612\n",
      "Cost after epoch 3400: 0.443994\n",
      "Cost after epoch 3500: 0.443415\n",
      "Cost after epoch 3600: 0.442865\n",
      "Cost after epoch 3700: 0.442353\n",
      "Cost after epoch 3800: 0.441871\n",
      "Cost after epoch 3900: 0.441412\n",
      "Cost after epoch 4000: 0.440973\n",
      "Cost after epoch 4100: 0.440556\n",
      "Cost after epoch 4200: 0.440154\n",
      "Cost after epoch 4300: 0.439768\n",
      "Cost after epoch 4400: 0.439339\n",
      "Cost after epoch 4500: 0.438945\n",
      "Cost after epoch 4600: 0.438568\n",
      "Cost after epoch 4700: 0.438200\n",
      "Cost after epoch 4800: 0.437718\n",
      "Cost after epoch 4900: 0.437277\n",
      "Cost after epoch 5000: 0.436857\n",
      "Cost after epoch 5100: 0.436453\n",
      "Cost after epoch 5200: 0.436057\n",
      "Cost after epoch 5300: 0.435666\n",
      "Cost after epoch 5400: 0.435098\n",
      "Cost after epoch 5500: 0.434385\n",
      "Cost after epoch 5600: 0.433735\n",
      "Cost after epoch 5700: 0.433174\n",
      "Cost after epoch 5800: 0.432656\n",
      "Cost after epoch 5900: 0.432189\n",
      "Cost after epoch 6000: 0.431734\n",
      "Cost after epoch 6100: 0.431293\n",
      "Cost after epoch 6200: 0.430871\n",
      "Cost after epoch 6300: 0.430442\n",
      "Cost after epoch 6400: 0.430020\n",
      "Cost after epoch 6500: 0.429605\n",
      "Cost after epoch 6600: 0.429197\n",
      "Cost after epoch 6700: 0.428798\n",
      "Cost after epoch 6800: 0.428402\n",
      "Cost after epoch 6900: 0.428008\n",
      "Cost after epoch 7000: 0.427616\n",
      "Cost after epoch 7100: 0.427226\n",
      "Cost after epoch 7200: 0.426843\n",
      "Cost after epoch 7300: 0.426463\n",
      "Cost after epoch 7400: 0.426085\n",
      "Cost after epoch 7500: 0.425708\n",
      "Cost after epoch 7600: 0.425332\n",
      "Cost after epoch 7700: 0.424956\n",
      "Cost after epoch 7800: 0.424582\n",
      "Cost after epoch 7900: 0.424160\n",
      "Cost after epoch 8000: 0.423738\n",
      "Cost after epoch 8100: 0.423336\n",
      "Cost after epoch 8200: 0.422965\n",
      "Cost after epoch 8300: 0.422603\n",
      "Cost after epoch 8400: 0.422248\n",
      "Cost after epoch 8500: 0.421895\n",
      "Cost after epoch 8600: 0.421545\n",
      "Cost after epoch 8700: 0.421202\n",
      "Cost after epoch 8800: 0.420861\n",
      "Cost after epoch 8900: 0.420522\n",
      "Cost after epoch 9000: 0.420151\n",
      "Cost after epoch 9100: 0.419453\n",
      "Cost after epoch 9200: 0.418829\n",
      "Cost after epoch 9300: 0.418276\n",
      "Cost after epoch 9400: 0.417636\n",
      "Cost after epoch 9500: 0.416943\n",
      "Cost after epoch 9600: 0.416369\n",
      "Cost after epoch 9700: 0.415898\n",
      "Cost after epoch 9800: 0.415449\n",
      "Cost after epoch 9900: 0.415015\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "layers_sizes = [11, 6, 1]\n",
    "activations = [\"leaky_relu\", \"sigmoid\"]\n",
    "epochs = 10000\n",
    "lr = 0.03\n",
    "alpha = 0.01\n",
    "weights = H_layer_NN(X_train, Y_train, layers_sizes, activations, epochs, lr, alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hXJXkxjFqLSo",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-c7ee6cb817e8d6d3",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### 10- Prediction on test data using weights of the H-layer Neural Network:\n",
    "We have provided the full function. All you have to do is to run the below cells to check the accuracy of your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YTTx38D5daur",
    "jupyter": {
     "outputs_hidden": true
    },
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-6168a7fcdc5a3999",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# do not change code here\n",
    "def predict(X, Y, weights, activations, alpha=0.1, mode=\"Test\"):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "        X - Input data/examples you would like to predict\n",
    "        Y - Ground truth labels corresponding to input data X\n",
    "        weights - weights of the trained model\n",
    "        activations - A list (of length H), where each element is a Python string representing the activation function to be used in the corresponding layer\n",
    "        alpha - A hyperparameter for activation functions leaky_relu and ELU.\n",
    "    Outputs:\n",
    "        p -- predictions for the given dataset X\n",
    "    \"\"\"\n",
    "    m = X.shape[1]\n",
    "    p = np.zeros(m)\n",
    "    AH, _ = forward_module(X, weights, activations, alpha)\n",
    "    AH = np.squeeze(AH)\n",
    "    pos_pred = AH > 0.5\n",
    "    p[pos_pred] = 1\n",
    "    print(mode + \" Accuracy: \"  + str(np.sum(p == Y)/m))\n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "k7DZuhLCyaPk",
    "jupyter": {
     "outputs_hidden": true
    },
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-e6e9c3bc88d7fe38",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.8033333333333333\n",
      "Test Accuracy: 0.8923076923076924\n"
     ]
    }
   ],
   "source": [
    "training_predictions = predict(X_train, Y_train, weights, activations, mode=\"Train\")\n",
    "test_predictions = predict(X_test, Y_test, weights, activations, mode=\"Test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0Tcq0qXXGsGM",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-7d40f835821a378c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### 11-  Summary:\n",
    "* Finally, we have implemented a custom neural network with as many layers as you want and with each layer having an activation function of your choice. Congrats on completing such a lengthy assignment (with maybe a bit more math than expected !). We feel this will help you understand important NumPy concepts like broadcasting, masking, vectorisation, etc. and also give you an insight into data preprocessing. Also, we hope this will give you a feel of how backpropgation is implemented for a fully connected neural network.\n",
    "\n",
    "* You can always try and improve the accuracy of your model by tuning various hyperparameters like number of hidden layers, sizes of hidden layers, activation functions used in the hidden layers, learning rate, number of epochs, etc.\n",
    "\n",
    "* As you will learn later in the course, you can also employ regularization techniques (like L2 regularization and dropout regularization) to reduce overfitting and batch normalisation (useful for a very deep neural network)."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Create Assignment",
  "colab": {
   "collapsed_sections": [
    "hdWn_tEvflV7",
    "1PGh3P3sflW3",
    "f6RkICB7flYJ"
   ],
   "name": "Assignment_1_Parth_v3.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
